# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/03_losses.ipynb.

# %% auto 0
__all__ = ['MSELoss', 'L1Loss', 'CombinedLoss', 'MSSSIMLoss', 'MSSSIML1Loss', 'MSSSIML2Loss', 'CrossEntropyLossFlat3D',
           'DiceLoss', 'FRCLoss', 'FCRCutoff']

# %% ../nbs/03_losses.ipynb 3
from .core import store_attr

# %% ../nbs/03_losses.ipynb 4
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import sigmoid

from monai.losses import SSIMLoss

from scipy.optimize import curve_fit
from fastai.vision.all import mse, mae, CrossEntropyLossFlat, Any

from .metrics import FRCMetric, get_fourier_ring_correlations
from .core import torchTensor


# %% ../nbs/03_losses.ipynb 5
def MSELoss(
    inp: Any,
    targ: Any
    ) -> torchTensor:
    
    return mse(inp, targ)


# %% ../nbs/03_losses.ipynb 6
def L1Loss(
    inp: Any,
    targ: Any
    ) -> torchTensor:
    
    return mae(inp, targ)

# %% ../nbs/03_losses.ipynb 9
class CombinedLoss:
    "losses combined"
    def __init__(self, spatial_dims=2, mse_weight=0.33, mae_weight=0.33):
        store_attr()
        self.SSIM_loss = SSIMLoss(spatial_dims=spatial_dims)
        self.MSE_loss =  nn.MSELoss()
        self.MAE_loss =  nn.L1Loss()
        
    def __call__(self, pred, targ):
        return (1 - self.mse_weight - self.mae_weight) * self.SSIM_loss(pred, targ) + self.mse_weight * self.MSE_loss(pred, targ) + self.mae_weight * self.MAE_loss(pred, targ)
        

# %% ../nbs/03_losses.ipynb 10
class MSSSIMLoss(torch.nn.Module):
    def __init__(self, spatial_dims=2, # Number of spatial dimensions.
                 window_size: int = 8, # Size of the Gaussian filter for SSIM.
                 sigma: float = 1.5, # Standard deviation of the Gaussian filter.
                 reduction: str = "mean", # Specifies the reduction to apply to the output ('mean', 'sum', or 'none').
                 levels: int = 3, # Number of scales to use for MS-SSIM.
                 weights=None, # Weights to apply to each scale. If None, default values are used.
                 ):
        """
        Multi-Scale Structural Similarity (MSSSIM) Loss using MONAI's SSIMLoss as the base.

        """
        super(MSSSIMLoss, self).__init__()
        self.ssim = SSIMLoss(spatial_dims, win_size=window_size, kernel_sigma=sigma, reduction="none")
        self.levels = levels
        if weights is None:
            # Default weights for 5 levels, typically used in MS-SSIM
            self.weights = torch.FloatTensor([0.0448, 0.2856, 0.3001, 0.2363, 0.1333])
        else:
            self.weights = torch.FloatTensor(weights)
        self.weights = self.weights[:levels]
        self.reduction = reduction
        self.spatial_dims = spatial_dims

    def forward(self, x, y):
        # Ensure input tensors are the same size
        if x.size() != y.size():
            raise ValueError("Input images must have the same dimensions.")
        
        # Make sure the weights match the number of levels
        if len(self.weights) != self.levels:
            raise ValueError(f"Number of weights ({len(self.weights)}) must match the number of levels ({self.levels}).")
        
        msssim_values = []
        for i in range(self.levels):
            # Compute SSIM at this scale
            ssim_value = self.ssim(x, y)
            msssim_values.append(ssim_value * self.weights[i])

            # Downsample images for the next scale, except at the last scale
            if i < self.levels - 1:
                pool = F.avg_pool2d if self.spatial_dims == 2 else F.avg_pool3d
                x = pool(x, kernel_size=2, stride=2)
                y = pool(y, kernel_size=2, stride=2)

        # Stack and sum weighted SSIM values from all scales
        msssim = torch.stack(msssim_values, dim=0).sum(dim=0)/self.weights.sum()

        # Apply reduction (mean or sum)
        if self.reduction == "mean":
            return msssim.mean()
        elif self.reduction == "sum":
            return msssim.sum()
        else:
            return msssim


# %% ../nbs/03_losses.ipynb 12
class MSSSIML1Loss(torch.nn.Module):
    def __init__(self, spatial_dims=2, # Number of spatial dimensions.
                 alpha: float = 0.025, #  Weighting factor between MS-SSIM and L1 loss.
                 window_size: int = 8, # Size of the Gaussian filter for SSIM.
                 sigma: float = 1.5, # Standard deviation of the Gaussian filter.
                 reduction: str = "mean", # Specifies the reduction to apply to the output ('mean', 'sum', or 'none').
                 levels: int = 3, # Number of scales to use for MS-SSIM.
                 weights=None, # Weights to apply to each scale. If None, default values are used.
                 ):
        """
        Multi-Scale Structural Similarity (MSSSIM) with Gaussian-weighted L1 Loss.

        """
        super(MSSSIML1Loss, self).__init__()
        self.msssim = MSSSIMLoss(spatial_dims=spatial_dims, window_size=window_size, sigma=sigma, 
                                 reduction="none", levels=levels, weights=weights)
        store_attr()

    def forward(self, x, y):
        # Compute MSSSIM loss
        msssim_loss = self.msssim(x, y)

        # Compute L1 loss with Gaussian weighting
        gaussian = self.get_gaussian_weight(x.size()).to(x.device)
        l1_loss = F.l1_loss(x, y, reduction='none') * gaussian

        # Adjust reduction to accommodate 3D
        spatial_dims = tuple(range(1, x.ndim))  # Automatically handles 2D and 3D

        if self.reduction == "mean":
            l1_loss = l1_loss.mean(dim=spatial_dims)
        elif self.reduction == "sum":
            l1_loss = l1_loss.sum(dim=spatial_dims)

        # Combine the two losses
        combined_loss = self.alpha * msssim_loss + (1 - self.alpha) * l1_loss

        if self.reduction == "mean":
            return combined_loss.mean()
        elif self.reduction == "sum":
            return combined_loss.sum()
        else:
            return combined_loss

    def get_gaussian_weight(self, size):
        """Generate a Gaussian weight tensor based on input size."""
        batch_size, channels, *spatial_shape = size
        spatial_dims = len(spatial_shape)
        
        if spatial_dims == 2:
            width, height = spatial_shape
            sigma = width / 6.0
            x, y = torch.arange(width, dtype=torch.float32, device='cuda'), torch.arange(height, dtype=torch.float32, device='cuda')
            center_x, center_y = (width - 1) / 2.0, (height - 1) / 2.0
            x_grid, y_grid = torch.meshgrid(x, y, indexing='ij')
            gaussian = torch.exp(-((x_grid - center_x)**2 + (y_grid - center_y)**2) / (2 * sigma**2))
            gaussian /= gaussian.sum()
            gaussian_weight = gaussian.view(1, 1, width, height).expand(batch_size, channels, -1, -1)

        elif spatial_dims == 3:
            depth, width, height = spatial_shape
            sigma = width / 6.0
            z = torch.arange(depth, dtype=torch.float32, device='cuda')
            x = torch.arange(width, dtype=torch.float32, device='cuda')
            y = torch.arange(height, dtype=torch.float32, device='cuda')
            center_z, center_x, center_y = (depth - 1) / 2.0, (width - 1) / 2.0, (height - 1) / 2.0
            z_grid, x_grid, y_grid = torch.meshgrid(z, x, y, indexing='ij')
            gaussian = torch.exp(-((z_grid - center_z)**2 + (x_grid - center_x)**2 + (y_grid - center_y)**2) / (2 * sigma**2))
            gaussian /= gaussian.sum()
            gaussian_weight = gaussian.view(1, 1, depth, width, height).expand(batch_size, channels, -1, -1, -1)

        return gaussian_weight


# %% ../nbs/03_losses.ipynb 14
class MSSSIML2Loss(torch.nn.Module):
    def __init__(self, spatial_dims=2, # Number of spatial dimensions.
                 alpha: float = 0.1,# Weighting factor between MS-SSIM and L2 loss.
                 window_size: int = 11,# Size of the Gaussian window for SSIM.
                 sigma: float = 1.5,# Standard deviation of the Gaussian.
                 reduction: str = "mean",# Specifies the reduction to apply to the output ('mean', 'sum', or 'none').
                 levels: int = 3,# Number of scales to use for MS-SSIM.
                 weights=None,# Weights to apply to each scale. If None, default values are used.
                 ):
        """
        Multi-Scale Structural Similarity (MSSSIM) with Gaussian-weighted L2 Loss.

        """
        super(MSSSIML2Loss, self).__init__()
        self.msssim = MSSSIMLoss(spatial_dims=spatial_dims, window_size=window_size, sigma=sigma, reduction="none", levels=levels, weights=weights)
        self.alpha = alpha
        self.reduction = reduction
        self.window_size = window_size
        self.sigma = sigma

    def forward(self, x, y):
        # Compute MSSSIM loss
        msssim_loss = self.msssim(x, y)

        # Compute L1 loss with Gaussian weighting
        # Generate Gaussian kernel based on the input size
        batch_size, _, height, width = x.size()
        gaussian = self.get_gaussian_weight(x.size()).to(x.device)

        # Apply the Gaussian kernel as a weight to the L1 loss
        l2_loss = F.mse_loss(x, y, reduction='none')
        l2_loss = l2_loss * gaussian

        # Sum or average the L1 loss based on the reduction
        if self.reduction == "mean":
            l2_loss = l2_loss.mean(dim=(1, 2, 3))  # Reduce over all spatial dimensions
        elif self.reduction == "sum":
            l2_loss = l2_loss.sum(dim=(1, 2, 3))   # Sum over all spatial dimensions
        else:
            l2_loss = l2_loss  # No reduction if 'none' is specified

        # Combine the two losses
        combined_loss = self.alpha * msssim_loss + (1 - self.alpha) * l2_loss

        # Apply final reduction to the combined loss
        if self.reduction == "mean":
            return combined_loss.mean()
        elif self.reduction == "sum":
            return combined_loss.sum()
        else:
            return combined_loss
        
    def get_gaussian_weight(self, size):
        """Generate a Gaussian weight tensor based on input size."""
        batch_size, channels, width, height = size
        sigma = width / 6.0  # Using width/6 as an approximate scale for sigma

        x = torch.arange(width, dtype=torch.float32, device='cuda')
        y = torch.arange(height, dtype=torch.float32, device='cuda')

        # Handle even-sized patches by adjusting the center position calculation
        center_x = (width - 1) / 2.0 if width % 2 == 1 else width / 2.0
        center_y = (height - 1) / 2.0 if height % 2 == 1 else height / 2.0

        # Explicitly pass the indexing argument
        x_grid, y_grid = torch.meshgrid(x, y, indexing='ij')

        gaussian = torch.exp(-((x_grid - center_x)**2 + (y_grid - center_y)**2) / (2 * sigma**2))
        gaussian /= gaussian.sum()  # Normalize the Gaussian

        gaussian_weight = gaussian.view(1, 1, width, height)
        gaussian_weight = gaussian_weight.expand(batch_size, channels, -1, -1)
        return gaussian_weight

# %% ../nbs/03_losses.ipynb 17
class CrossEntropyLossFlat3D(CrossEntropyLossFlat):
    "Same as `nn.CrossEntropyLoss`, but flattens input and target for 3D inputs."
    def __call__(self, 
        inp: torchTensor, # Predictions (e.g., NCDHW or similar format)
        targ: torchTensor, # Targets
        **kwargs
    ) -> torchTensor:
        "Flatten spatial dimensions (DHW) and apply loss."
        inp = inp.permute(0, 2, 3, 4, 1).contiguous()  # Move class axis to the end
        targ = targ.contiguous()
        if self.flatten:
            inp = inp.view(-1, inp.shape[-1])
            targ = targ.view(-1)
        return self.func(inp, targ, **kwargs)

# %% ../nbs/03_losses.ipynb 18
class DiceLoss(nn.Module):

    """
    DiceLoss computes the Sørensen–Dice coefficient loss, which is often used 
    for evaluating the performance of image segmentation algorithms.

    The Dice coefficient is a measure of overlap between two samples. It ranges 
    from 0 (no overlap) to 1 (perfect overlap). The Dice loss is computed as 
    1 - Dice coefficient, so it ranges from 1 (no overlap) to 0 (perfect overlap).

    Attributes:
        smooth (float): A smoothing factor to avoid division by zero and ensure numerical stability.

    Methods:
        forward(inputs, targets):
            Computes the Dice loss between the predicted probabilities (inputs) 
            and the ground truth (targets).
    """

    def __init__(self, smooth=1, # Smoothing factor to avoid division by zero
                 ):

        """
        Initializes the DiceLoss instance with a smoothing factor.

        """
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, inputs, targets):
        
        # Make sure the inputs are probabilities
        inputs = sigmoid(inputs)

        # Flatten tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)

        # Calculate the intersection
        intersection = (inputs * targets).sum()

        # Compute Dice Coefficient
        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)

        # Copmute dice loss
        loss = 1 - dice

        return loss
        

# %% ../nbs/03_losses.ipynb 22
def FRCLoss(image1,# The first input image.
            image2,# The second input image.
            ):

    """
    Compute the Fourier Ring Correlation (FRC) loss between two images.

    Returns:
        - torch.Tensor: The FRC loss.
    """
    
    return (1 - FRCMetric(image1, image2))
    

# %% ../nbs/03_losses.ipynb 23
def FCRCutoff(image1,# The first input image.
             image2,# The second input image.
             ):


    """
    Calculate the cutoff frequency at when Fourier ring correlation drops to 1/7.

    Returns:
        - float: The cutoff frequency.
    """

    # Get y and x coordinates
    y, x = get_fourier_ring_correlations(image1, image2)

    # x -> frequency   y -> correlation
    x = x.numpy()
    y = y.numpy()


    # Exponential function to fit
    def exponential_func(x, a, b, c):
        return a * np.exp(-b * x) + c

    # Make fit
    params, _ = curve_fit(exponential_func, x, y, p0=[1, 1, 1])

    # Get Cutoff requency at 1/7
    cutoff_frequency = (exponential_func((1/7), *params))

    return cutoff_frequency
