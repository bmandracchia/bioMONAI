[
  {
    "objectID": "nets.html",
    "href": "nets.html",
    "title": "Networks",
    "section": "",
    "text": "source\n\n\n\n create_custom_unet (resnet_version, output_channels, img_size=(128, 128),\n                     pretrained=True, n_in=1, cut=4)\n\n*Create a U-Net model with a ResNet backbone.\nReturns: - U-Net model with the specified ResNet backbone.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nresnet_version\n\n\nChoose a ResNet model between: ‘resnet18’, ‘resnet34’, ‘resnet50’, ‘resnet101’, and ‘resnet152’.\n\n\noutput_channels\n\n\nNumber of output channels.\n\n\nimg_size\ntuple\n(128, 128)\nTuple for the input image size, default is (128, 128).\n\n\npretrained\nbool\nTrue\nIf True, use a pretrained ResNet backbone.\n\n\nn_in\nint\n1\nNumber of input channels, default is 1 (e.g., grayscale).\n\n\ncut\nint\n4\nThe cut point for the ResNet model, default is 4."
  },
  {
    "objectID": "nets.html#unet",
    "href": "nets.html#unet",
    "title": "Networks",
    "section": "",
    "text": "source\n\n\n\n create_custom_unet (resnet_version, output_channels, img_size=(128, 128),\n                     pretrained=True, n_in=1, cut=4)\n\n*Create a U-Net model with a ResNet backbone.\nReturns: - U-Net model with the specified ResNet backbone.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nresnet_version\n\n\nChoose a ResNet model between: ‘resnet18’, ‘resnet34’, ‘resnet50’, ‘resnet101’, and ‘resnet152’.\n\n\noutput_channels\n\n\nNumber of output channels.\n\n\nimg_size\ntuple\n(128, 128)\nTuple for the input image size, default is (128, 128).\n\n\npretrained\nbool\nTrue\nIf True, use a pretrained ResNet backbone.\n\n\nn_in\nint\n1\nNumber of input channels, default is 1 (e.g., grayscale).\n\n\ncut\nint\n4\nThe cut point for the ResNet model, default is 4."
  },
  {
    "objectID": "nets.html#denoising-cnn",
    "href": "nets.html#denoising-cnn",
    "title": "Networks",
    "section": "Denoising CNN",
    "text": "Denoising CNN\n\nsource\n\nDnCNN\n\n DnCNN (spatial_dims=2, in_channels=1, out_channels=1, num_of_layers=9,\n        features=64, kernel_size=3)\n\nA Deep Neural Network for Image Denoising (DnCNN) model.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspatial_dims\nint\n2\nNumber of spatial dimensions\n\n\nin_channels\nint\n1\nNumber of input channels\n\n\nout_channels\nint\n1\nNumber of output channels\n\n\nnum_of_layers\nint\n9\nNumber of convolutional layers\n\n\nfeatures\nint\n64\nNumber of feature maps\n\n\nkernel_size\nint\n3\nSize of the convolution kernel\n\n\n\n\nx = torch_randn(16, 1, 32, 64)\n\ntst = DnCNN(2,1)\ntest_eq(tst(x).shape, x.shape)"
  },
  {
    "objectID": "nets.html#deeplab-v3",
    "href": "nets.html#deeplab-v3",
    "title": "Networks",
    "section": "DeepLab v3+",
    "text": "DeepLab v3+\n\nConfig\n\nsource\n\n\ninterpolate\n\n interpolate (x:torch.Tensor, size:Union[List[int],Tuple[int,...]],\n              dims:int)\n\n\n\n\n\nType\nDetails\n\n\n\n\nx\nTensor\nInput tensor\n\n\nsize\nUnion\nSize of the output tensor\n\n\ndims\nint\nNumber of spatial dimensions\n\n\nReturns\nTensor\nOutput tensor\n\n\n\n\nsource\n\n\nget_padding\n\n get_padding (kernel_size:int, dilation:int)\n\n\n\n\n\nType\nDetails\n\n\n\n\nkernel_size\nint\nSize of the convolution kernel\n\n\ndilation\nint\nDilation rate\n\n\nReturns\nint\nPadding size\n\n\n\n\nsource\n\n\nDeeplabConfig\n\n DeeplabConfig (dimensions:int, in_channels:int, out_channels:int,\n                backbone:str='xception', pretrained:bool=False,\n                middle_flow_blocks:int=16,\n                aspp_dilations:List[int]=&lt;factory&gt;,\n                entry_block3_stride:int=2, middle_block_dilation:int=1,\n                exit_block_dilations:Tuple[int,int]=(1, 2))\n\n\n\nBlocks\n\nsource\n\n\nBlock\n\n Block (config:__main__.DeeplabConfig, inplanes:int, planes:int, reps:int,\n        stride:int=1, dilation:int=1, start_with_relu:bool=True,\n        grow_first:bool=True, is_last:bool=False)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\nDeeplabConfig\n\nConfiguration for the Deeplab model\n\n\ninplanes\nint\n\nNumber of input channels\n\n\nplanes\nint\n\nNumber of output channels\n\n\nreps\nint\n\nNumber of convolutional layers\n\n\nstride\nint\n1\nStride for the convolution\n\n\ndilation\nint\n1\nDilation rate for the convolution\n\n\nstart_with_relu\nbool\nTrue\nIf True, start with a ReLU activation\n\n\ngrow_first\nbool\nTrue\nIf True, increase the number of channels in the first convolution\n\n\nis_last\nbool\nFalse\nIf True, add a convolution layer at the end\n\n\n\n\nsource\n\n\nSeparableConv\n\n SeparableConv (config:__main__.DeeplabConfig, inplanes:int, planes:int,\n                kernel_size:int=3, stride:int=1, dilation:int=1,\n                bias:bool=False, norm:Optional[str]=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDefault\nDetails\n\n\n\n\nconfig\nDeeplabConfig\n\nConfiguration for the Deeplab model\n\n\ninplanes\nint\n\nNumber of input channels\n\n\nplanes\nint\n\nNumber of output channels\n\n\nkernel_size\nint\n3\nSize of the convolution kernel\n\n\nstride\nint\n1\nStride for the convolution\n\n\ndilation\nint\n1\nDilation rate for the convolution\n\n\nbias\nbool\nFalse\nIf True, add a bias term\n\n\nnorm\nOptional\nNone\nType of normalization layer\n\n\n\n\n\nAligned Xception\n\nsource\n\n\nXception\n\n Xception (config:__main__.DeeplabConfig)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDetails\n\n\n\n\nconfig\nDeeplabConfig\nConfiguration for the Deeplab model\n\n\n\n\n\nASPP\n\nsource\n\n\nASPP_module\n\n ASPP_module (config:__main__.DeeplabConfig, inplanes:int, planes:int,\n              dilation:int)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nType\nDetails\n\n\n\n\nconfig\nDeeplabConfig\nConfiguration for the Deeplab model\n\n\ninplanes\nint\nNumber of input channels\n\n\nplanes\nint\nNumber of output channels\n\n\ndilation\nint\nDilation rate for the convolution\n\n\n\n\n\nDeepLab V3\n\nsource\n\n\nDeeplab\n\n Deeplab (config:__main__.DeeplabConfig)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\nExample\n\n# Load a pre-trained ResNet backbone\nresnet_backbone = ResNetFeatures('resnet10', pretrained=False, in_channels=1, spatial_dims=3)\n\n# Forward pass through the backbone to get the output before the final classifier\ndummy_input = torch_randn(1, 1, 64, 224, 224)  # Example input size; adjust based on your needs\noutput = resnet_backbone(dummy_input)\n\n# The shape of 'output' will give you the number of channels at this stage in the backbone\nprint(\"Output channels:\", output[-1].shape[1])\n\nOutput channels: 512\n\n\n\n# For 2D images\nconfig_2d = DeeplabConfig(\n    dimensions=2,\n    in_channels=3,  # For RGB images\n    out_channels=4,\n    backbone=\"xception\",  # or whatever backbone you're using\n    aspp_dilations=[1, 6, 12, 18]\n)\nmodel_2d = Deeplab(config_2d)\n\n# For 3D images\nconfig_3d = DeeplabConfig(\n    dimensions=3,\n    in_channels=1,  # For single-channel 3D medical images\n    out_channels=4,\n    middle_flow_blocks=16,\n    aspp_dilations=[1, 6, 12, 18]\n)\nmodel_3d = Deeplab(config_3d)\n\n\nfrom torch import no_grad as torch_no_grad\n\n\ndef test_deeplab(config, input_shape, expected_output_shape):\n    set_determinism(0)  # For reproducibility\n    \n    model = Deeplab(config)\n    model.eval()  # Set the model to evaluation mode\n    \n    # Generate random input tensor\n    x = torch_randn(*input_shape)\n    \n    # Forward pass\n    with torch_no_grad():\n        output = model(x)\n    \n    # Check output shape\n    assert output.shape == expected_output_shape, f\"Expected shape {expected_output_shape}, but got {output.shape}\"\n    \n    print(f\"Test passed for {config.dimensions}D model with backbone {config.backbone}\")\n    print(f\"Input shape: {input_shape}\")\n    print(f\"Output shape: {output.shape}\")\n    print(\"---\")\n\n\n# Test 2D model\nconfig_2d = DeeplabConfig(\n    dimensions=2,\n    in_channels=3,\n    out_channels=4,\n    backbone=\"xception\",\n    aspp_dilations=[1, 6, 12, 18]\n)\ntest_deeplab(config_2d, (1, 3, 64, 64), (1, 4, 64, 64))\n\n# Test 2D model with ResNet50 backbone\nconfig_2d_resnet = DeeplabConfig(\n    dimensions=2,\n    in_channels=3,\n    out_channels=4,\n    backbone=\"resnet50\",\n    aspp_dilations=[1, 6, 12, 18]\n)\ntest_deeplab(config_2d_resnet, (1, 3, 64, 64), (1, 4, 64, 64))\n\n# Test 3D model\nconfig_3d = DeeplabConfig(\n    dimensions=3,\n    in_channels=1,\n    out_channels=4,\n    backbone=\"xception\",\n    aspp_dilations=[1, 6, 12, 18]\n)\ntest_deeplab(config_3d, (1, 1, 64, 64, 64), (1, 4, 64, 64, 64))\n\n# Test 3D model with ResNet10 backbone\nconfig_3d_resnet = DeeplabConfig(\n    dimensions=3,\n    in_channels=1,\n    out_channels=4,\n    backbone=\"resnet10\",\n    aspp_dilations=[1, 6, 12, 18]\n)\ntest_deeplab(config_3d_resnet, (1, 1, 64, 64, 64), (1, 4, 64, 64, 64))\n\nprint(\"All tests passed successfully!\")\n\nTest passed for 2D model with backbone xception\nInput shape: (1, 3, 64, 64)\nOutput shape: torch.Size([1, 4, 64, 64])\n---\nTest passed for 2D model with backbone resnet50\nInput shape: (1, 3, 64, 64)\nOutput shape: torch.Size([1, 4, 64, 64])\n---\nTest passed for 3D model with backbone xception\nInput shape: (1, 1, 64, 64, 64)\nOutput shape: torch.Size([1, 4, 64, 64, 64])\n---\nTest passed for 3D model with backbone resnet10\nInput shape: (1, 1, 64, 64, 64)\nOutput shape: torch.Size([1, 4, 64, 64, 64])\n---\nAll tests passed successfully!"
  },
  {
    "objectID": "nets.html#umamba",
    "href": "nets.html#umamba",
    "title": "Networks",
    "section": "UMamba",
    "text": "UMamba\n\nsource\n\nMambaLayer\n\n MambaLayer (dim, d_state=16, d_conv=4, expand=2)\n\nA custom neural network layer that incorporates the Mamba block from the Mamba model, along with layer normalization and optional mixed precision handling.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndim\n\n\nDimension of the input tensor\n\n\nd_state\nint\n16\nExpansion factor for the state in the Mamba block\n\n\nd_conv\nint\n4\nWidth of the local convolution in the Mamba block\n\n\nexpand\nint\n2\nFactor by which to expand the dimensions in the Mamba block\n\n\n\n\nsource\n\n\nUMamba\n\n UMamba (spatial_dims:int, in_channels:int, out_channels:int,\n         kernel_size:Sequence[Union[Sequence[int],int]],\n         strides:Sequence[Union[Sequence[int],int]],\n         upsample_kernel_size:Sequence[Union[Sequence[int],int]],\n         filters:Optional[Sequence[int]]=None,\n         dropout:Union[Tuple,str,float,NoneType]=None,\n         norm_name:Union[Tuple,str]=('INSTANCE', {'affine': True}),\n         act_name:Union[Tuple,str]=('leakyrelu', {'inplace': True,\n         'negative_slope': 0.01}), deep_supervision:bool=False,\n         deep_supr_num:int=1, res_block:bool=False, trans_bias:bool=False)\n\n*A custom subclass of DynUNet that integrates the Mamba layer into the model’s bottleneck.\nThis class inherits from DynUNet and adds a specific bottleneck structure containing a convolution block followed by a MambaLayer.*\n\nExample\n\nx = torch_randn(16, 1, 32, 64)\n\ntst = DynUNet(2,1,1,[3,3,3],[1,1,1],[1,1])\nprint(tst(x).shape)\ntest_eq(tst(x).shape, x.shape)\n\ntorch.Size([16, 1, 32, 64])\n\n\n\nx = torch_randn(16, 1, 32, 64).cuda()\n\ntst = UMamba(2,1,1,[3,3,3],[1,1,1],[1,1]).cuda()\nprint(tst(x).shape)\ntest_eq(tst(x).shape, x.shape)\n\ntorch.Size([16, 1, 32, 64])"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core",
    "section": "",
    "text": "This section includes essential imports used throughout the core library, providing foundational tools for data handling, model training, and evaluation. Key imports cover areas such as data blocks, data loaders, custom loss functions, optimizers, callbacks, and logging.\n\nsource\n\n\n\n DataBlock (blocks:list=None, dl_type:TfmdDL=None, getters:list=None,\n            n_inp:int=None, item_tfms:list=None, batch_tfms:list=None,\n            get_items=None, splitter=None, get_y=None, get_x=None)\n\nGeneric container to quickly build Datasets and DataLoaders.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nblocks\nlist\nNone\nOne or more TransformBlocks\n\n\ndl_type\nTfmdDL\nNone\nTask specific TfmdDL, defaults to block’s dl_type orTfmdDL\n\n\ngetters\nlist\nNone\nGetter functions applied to results of get_items\n\n\nn_inp\nint\nNone\nNumber of inputs\n\n\nitem_tfms\nlist\nNone\nItemTransforms, applied on an item\n\n\nbatch_tfms\nlist\nNone\nTransforms or RandTransforms, applied by batch\n\n\nget_items\nNoneType\nNone\n\n\n\nsplitter\nNoneType\nNone\n\n\n\nget_y\nNoneType\nNone\n\n\n\nget_x\nNoneType\nNone\n\n\n\n\nThe DataBlock class acts as a container for creating data processing pipelines, allowing easy customization of datasets and data loaders. It enables the definition of item transformations, batch transformations, and dataset split methods, streamlining data preprocessing and loading across various stages of model training.\n\nsource\n\n\n\n\n DataLoaders (*loaders, path:str|pathlib.Path='.', device=None)\n\nBasic wrapper around several DataLoaders.\nThe DataLoaders class is a container for managing training and validation datasets. This class wraps one or more DataLoader instances, ensuring seamless data management and transfer across devices (CPU or GPU) for efficient training and evaluation.\n\nsource\n\n\n\nGroup together a model, some dls and a loss_func to handle training\nThe Learner class is the main interface for training machine learning models, encapsulating the model, data, loss function, optimizer, and training metrics. It simplifies the training process by providing built-in functionality for model evaluation, hyperparameter tuning, and training loop customization, allowing you to focus on model optimization.\n\nsource\n\n\n\n\n ShowGraphCallback (after_create=None, before_fit=None, before_epoch=None,\n                    before_train=None, before_batch=None, after_pred=None,\n                    after_loss=None, before_backward=None,\n                    after_cancel_backward=None, after_backward=None,\n                    before_step=None, after_cancel_step=None,\n                    after_step=None, after_cancel_batch=None,\n                    after_batch=None, after_cancel_train=None,\n                    after_train=None, before_validate=None,\n                    after_cancel_validate=None, after_validate=None,\n                    after_cancel_epoch=None, after_epoch=None,\n                    after_cancel_fit=None, after_fit=None)\n\nUpdate a graph of training and validation loss\nThe ShowGraphCallback is a convenient callback for visualizing training progress. By plotting the training and validation loss, it helps users monitor convergence and performance, making it easy to assess if the model requires adjustments in learning rate, architecture, or data handling.\n\nsource\n\n\n\n\n CSVLogger (fname='history.csv', append=False)\n\nLog the results displayed in learn.path/fname\nThe CSVLogger is a tool for logging model training metrics to a CSV file, offering a permanent record of training history. This feature is especially useful for long-term experiments and fine-tuning, allowing you to track and analyze model performance over time.\n\n\n\n\n\n cells3d ()\n\n*3D fluorescence microscopy image of cells.\nThe returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.*\nThe cells3d function returns a sample 3D fluorescence microscopy image. This is a valuable test image for demonstration and analysis, consisting of both cell membrane and nucleus channels. It can serve as a default dataset for evaluating and benchmarking new models and transformations."
  },
  {
    "objectID": "core.html#imports",
    "href": "core.html#imports",
    "title": "Core",
    "section": "",
    "text": "This section includes essential imports used throughout the core library, providing foundational tools for data handling, model training, and evaluation. Key imports cover areas such as data blocks, data loaders, custom loss functions, optimizers, callbacks, and logging.\n\nsource\n\n\n\n DataBlock (blocks:list=None, dl_type:TfmdDL=None, getters:list=None,\n            n_inp:int=None, item_tfms:list=None, batch_tfms:list=None,\n            get_items=None, splitter=None, get_y=None, get_x=None)\n\nGeneric container to quickly build Datasets and DataLoaders.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nblocks\nlist\nNone\nOne or more TransformBlocks\n\n\ndl_type\nTfmdDL\nNone\nTask specific TfmdDL, defaults to block’s dl_type orTfmdDL\n\n\ngetters\nlist\nNone\nGetter functions applied to results of get_items\n\n\nn_inp\nint\nNone\nNumber of inputs\n\n\nitem_tfms\nlist\nNone\nItemTransforms, applied on an item\n\n\nbatch_tfms\nlist\nNone\nTransforms or RandTransforms, applied by batch\n\n\nget_items\nNoneType\nNone\n\n\n\nsplitter\nNoneType\nNone\n\n\n\nget_y\nNoneType\nNone\n\n\n\nget_x\nNoneType\nNone\n\n\n\n\nThe DataBlock class acts as a container for creating data processing pipelines, allowing easy customization of datasets and data loaders. It enables the definition of item transformations, batch transformations, and dataset split methods, streamlining data preprocessing and loading across various stages of model training.\n\nsource\n\n\n\n\n DataLoaders (*loaders, path:str|pathlib.Path='.', device=None)\n\nBasic wrapper around several DataLoaders.\nThe DataLoaders class is a container for managing training and validation datasets. This class wraps one or more DataLoader instances, ensuring seamless data management and transfer across devices (CPU or GPU) for efficient training and evaluation.\n\nsource\n\n\n\nGroup together a model, some dls and a loss_func to handle training\nThe Learner class is the main interface for training machine learning models, encapsulating the model, data, loss function, optimizer, and training metrics. It simplifies the training process by providing built-in functionality for model evaluation, hyperparameter tuning, and training loop customization, allowing you to focus on model optimization.\n\nsource\n\n\n\n\n ShowGraphCallback (after_create=None, before_fit=None, before_epoch=None,\n                    before_train=None, before_batch=None, after_pred=None,\n                    after_loss=None, before_backward=None,\n                    after_cancel_backward=None, after_backward=None,\n                    before_step=None, after_cancel_step=None,\n                    after_step=None, after_cancel_batch=None,\n                    after_batch=None, after_cancel_train=None,\n                    after_train=None, before_validate=None,\n                    after_cancel_validate=None, after_validate=None,\n                    after_cancel_epoch=None, after_epoch=None,\n                    after_cancel_fit=None, after_fit=None)\n\nUpdate a graph of training and validation loss\nThe ShowGraphCallback is a convenient callback for visualizing training progress. By plotting the training and validation loss, it helps users monitor convergence and performance, making it easy to assess if the model requires adjustments in learning rate, architecture, or data handling.\n\nsource\n\n\n\n\n CSVLogger (fname='history.csv', append=False)\n\nLog the results displayed in learn.path/fname\nThe CSVLogger is a tool for logging model training metrics to a CSV file, offering a permanent record of training history. This feature is especially useful for long-term experiments and fine-tuning, allowing you to track and analyze model performance over time.\n\n\n\n\n\n cells3d ()\n\n*3D fluorescence microscopy image of cells.\nThe returned data is a 3D multichannel array with dimensions provided in (z, c, y, x) order. Each voxel has a size of (0.29 0.26 0.26) micrometer. Channel 0 contains cell membranes, channel 1 contains nuclei.*\nThe cells3d function returns a sample 3D fluorescence microscopy image. This is a valuable test image for demonstration and analysis, consisting of both cell membrane and nucleus channels. It can serve as a default dataset for evaluating and benchmarking new models and transformations."
  },
  {
    "objectID": "core.html#engine",
    "href": "core.html#engine",
    "title": "Core",
    "section": "Engine",
    "text": "Engine\nThe engine module provides advanced functionalities for model training, including configurable training loops and evaluation functions tailored for bioinformatics applications.\n\nsource\n\nfastTrainer\n\n fastTrainer (dataloaders:fastai.data.core.DataLoaders, model:&lt;built-\n              infunctioncallable&gt;, loss_fn:typing.Any|None=None, optimizer\n              :fastai.optimizer.Optimizer|fastai.optimizer.OptimWrapper=&lt;f\n              unction Adam&gt;, lr:float|slice=0.001, splitter:&lt;built-\n              infunctioncallable&gt;=&lt;function trainable_params&gt;, callbacks:U\n              nion[fastai.callback.core.Callback,MutableSequence,NoneType]\n              =None, metrics:Union[Any,MutableSequence,NoneType]=None,\n              csv_log:bool=False, show_graph:bool=True,\n              show_summary:bool=False, find_lr:bool=False,\n              find_lr_fn=&lt;function valley&gt;,\n              path:str|pathlib.Path|None=None,\n              model_dir:str|pathlib.Path='models', wd:float|int|None=None,\n              wd_bn_bias:bool=False, train_bn:bool=True, moms:tuple=(0.95,\n              0.85, 0.95), default_cbs:bool=True)\n\nA custom implementation of the FastAI Learner class for training models in bioinformatics applications.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataloaders\nDataLoaders\n\nThe DataLoader objects containing training and validation datasets.\n\n\nmodel\ncallable\n\nA callable model that will be trained on the dataset.\n\n\nloss_fn\ntyping.Any | None\nNone\nThe loss function to optimize during training. If None, defaults to a suitable default.\n\n\noptimizer\nfastai.optimizer.Optimizer | fastai.optimizer.OptimWrapper\nAdam\nThe optimizer function to use. Defaults to Adam if not specified.\n\n\nlr\nfloat | slice\n0.001\nLearning rate for the optimizer. Can be a float or a slice object for learning rate scheduling.\n\n\nsplitter\ncallable\ntrainable_params\n\n\n\ncallbacks\nUnion\nNone\nA callable that determines which parameters of the model should be updated during training.\n\n\nmetrics\nUnion\nNone\nOptional list of callback functions to customize training behavior.\n\n\ncsv_log\nbool\nFalse\nMetrics to evaluate the performance of the model during training.\n\n\nshow_graph\nbool\nTrue\nWhether to log training history to a CSV file. If True, logs will be appended to ‘history.csv’.\n\n\nshow_summary\nbool\nFalse\nThe base directory where models are saved or loaded from. Defaults to None.\n\n\nfind_lr\nbool\nFalse\nSubdirectory within the base path where trained models are stored. Default is ‘models’.\n\n\nfind_lr_fn\nfunction\nvalley\nWeight decay factor for optimization. Defaults to None.\n\n\npath\nstr | pathlib.Path | None\nNone\nWhether to apply weight decay to batch normalization and bias parameters.\n\n\nmodel_dir\nstr | pathlib.Path\nmodels\nWhether to update the batch normalization statistics during training.\n\n\nwd\nfloat | int | None\nNone\n\n\n\nwd_bn_bias\nbool\nFalse\n\n\n\ntrain_bn\nbool\nTrue\n\n\n\nmoms\ntuple\n(0.95, 0.85, 0.95)\nTuple of tuples representing the momentum values for different layers in the model. Defaults to FastAI’s default settings if not specified.\n\n\ndefault_cbs\nbool\nTrue\nAutomatically include default callbacks such as ShowGraphCallback and CSVLogger.\n\n\n\n\nsource\n\n\nvisionTrainer\n\n visionTrainer (dataloaders:fastai.data.core.DataLoaders, model:&lt;built-\n                infunctioncallable&gt;, normalize=True, n_out=None,\n                pretrained=True, weights=None,\n                loss_fn:typing.Any|None=None, optimizer:fastai.optimizer.O\n                ptimizer|fastai.optimizer.OptimWrapper=&lt;function Adam&gt;,\n                lr:float|slice=0.001, splitter:&lt;built-\n                infunctioncallable&gt;=&lt;function trainable_params&gt;, callbacks\n                :Union[fastai.callback.core.Callback,MutableSequence,NoneT\n                ype]=None,\n                metrics:Union[Any,MutableSequence,NoneType]=None,\n                csv_log:bool=False, show_graph:bool=True,\n                show_summary:bool=False, find_lr:bool=False,\n                find_lr_fn=&lt;function valley&gt;,\n                path:str|pathlib.Path|None=None,\n                model_dir:str|pathlib.Path='models',\n                wd:float|int|None=None, wd_bn_bias:bool=False,\n                train_bn:bool=True, moms:tuple=(0.95, 0.85, 0.95),\n                default_cbs:bool=True, cut=None, init=&lt;function\n                kaiming_normal_&gt;, custom_head=None, concat_pool=True,\n                pool=True, lin_ftrs=None, ps=0.5, first_bn=True,\n                bn_final=False, lin_first=False, y_range=None, n_in=3)\n\nBuild a vision trainer from dataloaders and model\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataloaders\nDataLoaders\n\nThe DataLoader objects containing training and validation datasets.\n\n\nmodel\ncallable\n\nA callable model that will be trained on the dataset.\n\n\nnormalize\nbool\nTrue\n\n\n\nn_out\nNoneType\nNone\n\n\n\npretrained\nbool\nTrue\n\n\n\nweights\nNoneType\nNone\n\n\n\nloss_fn\ntyping.Any | None\nNone\nThe loss function to optimize during training. If None, defaults to a suitable default.\n\n\noptimizer\nfastai.optimizer.Optimizer | fastai.optimizer.OptimWrapper\nAdam\nThe optimizer function to use. Defaults to Adam if not specified.\n\n\nlr\nfloat | slice\n0.001\nLearning rate for the optimizer. Can be a float or a slice object for learning rate scheduling.\n\n\nsplitter\ncallable\ntrainable_params\n\n\n\ncallbacks\nUnion\nNone\nA callable that determines which parameters of the model should be updated during training.\n\n\nmetrics\nUnion\nNone\nOptional list of callback functions to customize training behavior.\n\n\ncsv_log\nbool\nFalse\nMetrics to evaluate the performance of the model during training.\n\n\nshow_graph\nbool\nTrue\nWhether to log training history to a CSV file. If True, logs will be appended to ‘history.csv’.\n\n\nshow_summary\nbool\nFalse\nThe base directory where models are saved or loaded from. Defaults to None.\n\n\nfind_lr\nbool\nFalse\nSubdirectory within the base path where trained models are stored. Default is ‘models’.\n\n\nfind_lr_fn\nfunction\nvalley\nWeight decay factor for optimization. Defaults to None.\n\n\npath\nstr | pathlib.Path | None\nNone\nWhether to apply weight decay to batch normalization and bias parameters.\n\n\nmodel_dir\nstr | pathlib.Path\nmodels\nWhether to update the batch normalization statistics during training.\n\n\nwd\nfloat | int | None\nNone\n\n\n\nwd_bn_bias\nbool\nFalse\n\n\n\ntrain_bn\nbool\nTrue\n\n\n\nmoms\ntuple\n(0.95, 0.85, 0.95)\nTuple of tuples representing the momentum values for different layers in the model. Defaults to FastAI’s default settings if not specified.\n\n\ndefault_cbs\nbool\nTrue\nAutomatically include default callbacks such as ShowGraphCallback and CSVLogger.\n\n\ncut\nNoneType\nNone\nmodel & head args\n\n\ninit\nfunction\nkaiming_normal_\n\n\n\ncustom_head\nNoneType\nNone\n\n\n\nconcat_pool\nbool\nTrue\n\n\n\npool\nbool\nTrue\n\n\n\nlin_ftrs\nNoneType\nNone\n\n\n\nps\nfloat\n0.5\n\n\n\nfirst_bn\nbool\nTrue\n\n\n\nbn_final\nbool\nFalse\n\n\n\nlin_first\nbool\nFalse\n\n\n\ny_range\nNoneType\nNone\n\n\n\nn_in\nint\n3"
  },
  {
    "objectID": "core.html#evaluation",
    "href": "core.html#evaluation",
    "title": "Core",
    "section": "Evaluation",
    "text": "Evaluation\n\nsource\n\ndisplay_statistics_table\n\n display_statistics_table (stats, fn_name='', as_dataframe=True)\n\nDisplay a table of the key statistics.\n\nsource\n\n\nplot_histogram_and_kde\n\n plot_histogram_and_kde (data, stats, bw_method=0.3, fn_name='')\n\nPlot the histogram and KDE of the data with key statistics marked.\n\nsource\n\n\nformat_sig\n\n format_sig (value)\n\nFormat numbers with two significant digits.\n\nsource\n\n\ncalculate_statistics\n\n calculate_statistics (data)\n\nCalculate key statistics for the data.\n\nsource\n\n\ncompute_metric\n\n compute_metric (predictions, targets, metric_fn)\n\nCompute the metric for each prediction-target pair. Handles cases where metric_fn has or does not have a ‘func’ attribute.\n\nsource\n\n\ncompute_losses\n\n compute_losses (predictions, targets, loss_fn)\n\nCompute the loss for each prediction-target pair.\n\nfrom numpy.random import standard_normal\n\n\na = standard_normal(1000)\n\nstats = calculate_statistics(a)\n\nplot_histogram_and_kde(a, stats)\n\n\n\n\n\nsource\n\n\nevaluate_model\n\n evaluate_model (trainer:fastai.learner.Learner,\n                 test_data:fastai.data.core.DataLoaders=None, loss=None,\n                 metrics=None, bw_method=0.3, show_graph=True,\n                 show_table=True, show_results=True, as_dataframe=True,\n                 cmap='magma')\n\nCalculate and optionally plot the distribution of loss values from predictions made by the trainer on test data, with an optional table of key statistics.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrainer\nLearner\n\nThe model trainer object with a get_preds method.\n\n\ntest_data\nDataLoaders\nNone\nDataLoader containing test data.\n\n\nloss\nNoneType\nNone\nLoss function to evaluate prediction-target pairs.\n\n\nmetrics\nNoneType\nNone\nSingle metric or a list of metrics to evaluate.\n\n\nbw_method\nfloat\n0.3\nBandwidth method for KDE.\n\n\nshow_graph\nbool\nTrue\nBoolean flag to show the histogram and KDE plot.\n\n\nshow_table\nbool\nTrue\nBoolean flag to show the statistics table.\n\n\nshow_results\nbool\nTrue\nBoolean flag to show model results on test data.\n\n\nas_dataframe\nbool\nTrue\nBoolean flag to display table as a DataFrame.\n\n\ncmap\nstr\nmagma\nColormap for visualization.\n\n\n\n\nsource\n\n\nevaluate_classification_model\n\n evaluate_classification_model (trainer:fastai.learner.Learner,\n                                test_data:fastai.data.core.DataLoaders=Non\n                                e, loss_fn=None, most_confused_n:int=1,\n                                normalize:bool=True, metrics=None,\n                                bw_method=0.3, show_graph=True,\n                                show_table=True, show_results=True,\n                                as_dataframe=True, cmap=&lt;matplotlib.colors\n                                .LinearSegmentedColormap object at\n                                0x7f57d42ab990&gt;)\n\nEvaluates a classification model by displaying results, confusion matrix, and most confused classes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrainer\nLearner\n\nThe trained model (learner) to evaluate.\n\n\ntest_data\nDataLoaders\nNone\nDataLoader with test data for evaluation. If None, the validation dataset is used.\n\n\nloss_fn\nNoneType\nNone\nLoss function used in the model for ClassificationInterpretation. If None, the loss function is loaded from trainer.\n\n\nmost_confused_n\nint\n1\nNumber of most confused class pairs to display.\n\n\nnormalize\nbool\nTrue\nWhether to normalize the confusion matrix.\n\n\nmetrics\nNoneType\nNone\nSingle metric or a list of metrics to evaluate.\n\n\nbw_method\nfloat\n0.3\nBandwidth method for KDE.\n\n\nshow_graph\nbool\nTrue\nBoolean flag to show the histogram and KDE plot.\n\n\nshow_table\nbool\nTrue\nBoolean flag to show the statistics table.\n\n\nshow_results\nbool\nTrue\nBoolean flag to show model results on test data.\n\n\nas_dataframe\nbool\nTrue\nBoolean flag to display table as a DataFrame.\n\n\ncmap\nLinearSegmentedColormap\n&lt;matplotlib.colors.LinearSegmentedColormap object at 0x7f57d42ab990&gt;\nColor map for the confusion matrix plot."
  },
  {
    "objectID": "core.html#utils",
    "href": "core.html#utils",
    "title": "Core",
    "section": "Utils",
    "text": "Utils\nThe utils module contains helper functions and classes to facilitate data manipulation, model setup, and training. These utilities add flexibility and convenience, supporting rapid experimentation and efficient data handling.\n\nsource\n\nattributesFromDict\n\n attributesFromDict (d)\n\nThe attributesFromDict function simplifies the conversion of dictionary keys and values into object attributes, allowing dynamic attribute creation for configuration objects. This utility is handy for initializing model or dataset configurations directly from dictionaries, improving code readability and maintainability.\n\nsource\n\n\nget_device\n\n get_device ()\n\n\nsource\n\n\nimg2float\n\n img2float (image, force_copy=False)\n\n\nsource\n\n\nimg2Tensor\n\n img2Tensor (image)"
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Datasets",
    "section": "",
    "text": "MedMNIST Datasets\n\nsource\n\n\ndownload_medmnist\n\n download_medmnist (dataset:str, output_dir:str='.',\n                    download_only:bool=False, save_images:bool=True)\n\n*Downloads the specified MedMNIST dataset and saves the training, validation, and test datasets into the specified output directory. Images are saved as .png for 2D data and multi-page .tiff for 3D data, organized into folders named after their labels.\nReturns: None, saves images in the specified output directory if save_images is True.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\nstr\n\nThe name of the MedMNIST dataset (e.g., ‘pathmnist’, ‘bloodmnist’, etc.).\n\n\noutput_dir\nstr\n.\nThe path to the directory where the datasets will be saved.\n\n\ndownload_only\nbool\nFalse\nIf True, only download the dataset into the output directory without processing.\n\n\nsave_images\nbool\nTrue\nIf True, save the images into the output directory as .png (2D datasets) or multipage .tiff (3D datasets) files.\n\n\n\n\nsource\n\n\nmedmnist2df\n\n medmnist2df (train_dataset, val_dataset=None, test_dataset=None,\n              mode='RGB')\n\n*Convert MedMNIST datasets to DataFrames, with images as PIL Image objects and labels as DataFrame columns.\nMissing datasets (if None) are represented by None in the return tuple.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntrain_dataset\n\n\nMedMNIST training dataset with images and labels\n\n\nval_dataset\nNoneType\nNone\n(Optional) MedMNIST validation dataset with images and labels\n\n\ntest_dataset\nNoneType\nNone\n(Optional) MedMNIST test dataset with images and labels\n\n\nmode\nstr\nRGB\nMode for PIL Image conversion, e.g., ‘RGB’, ‘L’\n\n\nReturns\n(&lt;class ‘pandas.core.frame.DataFrame’&gt;, &lt;class ‘pandas.core.frame.DataFrame’&gt;, &lt;class ‘pandas.core.frame.DataFrame’&gt;)\n\n(df_train, df_val, df_test): DataFrames with columns ‘image’ and ‘label’\n\n\n\n\n\nDownload data via Pooch\n\nsource\n\n\ndownload_file\n\n download_file (url, output_dir='data', extract=True, hash=None,\n                extract_dir=None)\n\nDownload and optionally decompress a single file using Pooch.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nurl\n\n\nThe URL of the file to be downloaded\n\n\noutput_dir\nstr\ndata\nThe directory where the downloaded file will be saved\n\n\nextract\nbool\nTrue\nIf True, decompresses the file if it’s in a compressed format\n\n\nhash\nNoneType\nNone\nOptional: You can add a checksum for integrity verification\n\n\nextract_dir\nNoneType\nNone\nDirectory to extract the files to\n\n\n\n\nsource\n\n\ndownload_dataset\n\n download_dataset (base_url, expected_checksums, file_names, output_dir,\n                   processor=None)\n\nDownload a dataset using Pooch and save it to the specified output directory.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nbase_url\n\n\nThe base URL from which the files will be downloaded.\n\n\nexpected_checksums\n\n\nA dictionary mapping file names to their expected checksums.\n\n\nfile_names\n\n\nA dictionary mapping task identifiers to file names.\n\n\noutput_dir\n\n\nThe directory where the downloaded files will be saved.\n\n\nprocessor\nNoneType\nNone\nA function to process the downloaded data.\n\n\n\n\nsource\n\n\ndownload_dataset_from_csv\n\n download_dataset_from_csv (csv_file, base_url, output_dir,\n                            processor=None, rows=None, prepend_mdf5=True)\n\nDownload a dataset using Pooch and save it to the specified output directory, reading file names and checksums from a CSV file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncsv_file\n\n\nPath to the CSV file containing file names and checksums.\n\n\nbase_url\n\n\nThe base URL from which the files will be downloaded.\n\n\noutput_dir\n\n\nThe directory where the downloaded files will be saved.\n\n\nprocessor\nNoneType\nNone\nA function to process the downloaded data.\n\n\nrows\nNoneType\nNone\nSpecific row indices to download. If None, download all rows.\n\n\nprepend_mdf5\nbool\nTrue\nIf True, prepend ‘md5:’ to the checksums.\n\n\n\n\n# Specify the directory where you want to save the downloaded files\noutput_directory = \"./_test_folder\"\n# Define the base URL for the MSD dataset\nbase_url = 'https://s3.ap-northeast-1.wasabisys.com/gigadb-datasets/live/pub/10.5524/100001_101000/100888/'\n\ndownload_dataset_from_csv('./data_examples/FMD_dataset_info.csv', base_url, output_directory, rows=[6])\n\nThe dataset has been successfully downloaded and saved to: ./_test_folder\n\n\n\n\nDownload data via Quilt/T4\nAllen Institute Cell Science (AICS)\n\nsource\n\n\naics_pipeline\n\n aics_pipeline (n_images_to_download=40, image_save_dir=None,\n                col='SourceReadPath')\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nn_images_to_download\nint\n40\nNumber of images to download\n\n\nimage_save_dir\nNoneType\nNone\nDirectory to save the images\n\n\ncol\nstr\nSourceReadPath\nColumn name for image paths in the data manifest\n\n\n\n\nimage_target_paths, data_manifest = aics_pipeline(1, \"../_data/aics\")\n\nLoading manifest: 100%|██████████| 77165/77165 [00:01&lt;00:00, 45.2k/s]\n\n\n\nprint(image_target_paths)\ndata_manifest.to_csv('../_data/aics/aics_dataset.csv')\n\n['../_data/aics/9e5d8f2e_3500001004_100X_20170623_5-Scene-1-P24-E06.czi_nucWholeIndexImageScale.tiff', '../_data/aics/77a69ff1_3500001004_100X_20170623_5-Scene-3-P26-F05.czi_nucWholeIndexImageScale.tiff']\n\n\n\nimage_target_paths, data_manifest = aics_pipeline(1, \"../_data/aics\", col=\"NucleusSegmentationReadPath\")\n\nLoading manifest: 100%|██████████| 77165/77165 [00:01&lt;00:00, 46.5k/s]\n100%|██████████| 491k/491k [00:02&lt;00:00, 171kB/s] \n\n\n\n\nDataset Manifest\nUtilities to make a list of all of the files of the train and test dataset in csv form.\n\nsource\n\n\nmanifest2csv\n\n manifest2csv (signal, target, paths=None, train_fraction=0.8,\n               data_save_path='./', train='train.csv', test='test.csv',\n               identifier=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsignal\n\n\nList of paths to signal images\n\n\ntarget\n\n\nList of paths to target images\n\n\npaths\nNoneType\nNone\nList of paths to images\n\n\ntrain_fraction\nfloat\n0.8\nFraction of data to use for training\n\n\ndata_save_path\nstr\n./\nPath to save the CSV files\n\n\ntrain\nstr\ntrain.csv\nName of the training CSV file\n\n\ntest\nstr\ntest.csv\nName of the test CSV file\n\n\nidentifier\nNoneType\nNone\nIdentifier to add to the paths\n\n\n\n\nmanifest2csv(data_manifest[\"ChannelNumberBrightfield\"],data_manifest[\"ChannelNumber405\"], image_target_paths, data_save_path='./data_examples/')\n\n\nsource\n\n\nsplit_dataframe\n\n split_dataframe (input_data, train_fraction=0.7, valid_fraction=0.1,\n                  split_column=None, stratify=False, add_is_valid=False,\n                  train_path='train.csv', test_path='test.csv',\n                  valid_path='valid.csv', data_save_path=None)\n\nSplits a DataFrame or CSV file into train, test, and optional validation sets.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_data\n\n\nPath to CSV file or DataFrame\n\n\ntrain_fraction\nfloat\n0.7\nProportion of data to use for the training set\n\n\nvalid_fraction\nfloat\n0.1\nProportion of data to use for the validation set\n\n\nsplit_column\nNoneType\nNone\nColumn name that indicates pre-defined split\n\n\nstratify\nbool\nFalse\nIf True, stratify by split_column during random split\n\n\nadd_is_valid\nbool\nFalse\nIf True, adds ‘is_valid’ column in the train set to mark validation samples\n\n\ntrain_path\nstr\ntrain.csv\nPath to save the training CSV file\n\n\ntest_path\nstr\ntest.csv\nPath to save the test CSV file\n\n\nvalid_path\nstr\nvalid.csv\nPath to save the validation CSV file\n\n\ndata_save_path\nNoneType\nNone\nPath to save the data files\n\n\n\n\nsource\n\n\nadd_columns_to_csv\n\n add_columns_to_csv (csv_path, column_data, output_path=None)\n\nAdds one or more new columns to an existing CSV file.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncsv_path\n\n\nPath to the input CSV file\n\n\ncolumn_data\n\n\nDictionary of column names and values to add. Each value can be a scalar (single value for all rows) or a list matching the number of rows.\n\n\noutput_path\nNoneType\nNone\nPath to save the updated CSV file. If None, it overwrites the input CSV file."
  },
  {
    "objectID": "visualize.html",
    "href": "visualize.html",
    "title": "Visualize",
    "section": "",
    "text": "# load sample data\ndata4D = img2float(cells3d())\ndata = data4D[:, 1, :, :] # load the nuclei channel\n\nDownloading file 'data/cells3d.tif' from 'https://gitlab.com/scikit-image/data/-/raw/master/cells3d.tif' to '/home/biagio/.var/app/com.visualstudio.code/cache/scikit-image/0.19.3'."
  },
  {
    "objectID": "visualize.html#display-2d-images",
    "href": "visualize.html#display-2d-images",
    "title": "Visualize",
    "section": "Display 2D images",
    "text": "Display 2D images\nFunction to quickly display 2D images\n\nsource\n\nplot_image\n\n plot_image (values)\n\nPlot a 2D image using Matplotlib. The function assumes that ‘values’ is a 2D array representing an image, typically in grayscale.\n\n\n\n\nDetails\n\n\n\n\nvalues\nA 2D array of pixel values representing the image.\n\n\n\n\n# Example usage:\nplot_image(data[35])"
  },
  {
    "objectID": "visualize.html#display-multichannel-images",
    "href": "visualize.html#display-multichannel-images",
    "title": "Visualize",
    "section": "Display Multichannel images",
    "text": "Display Multichannel images\nFunction to display RGB and multichannel images\n\nsource\n\nshow_multichannel\n\n show_multichannel (img, ax=None, figsize=None, title=None, max_slices=3,\n                    ctx=None, layout='horizontal', num_cols=3, cmap=None,\n                    norm=None, aspect=None, interpolation=None,\n                    alpha=None, vmin=None, vmax=None, origin=None,\n                    extent=None, interpolation_stage=None,\n                    filternorm=True, filterrad=4.0, resample=None,\n                    url=None, data=None, **kwargs)\n\nShow multi-channel CYX image with options for horizontal, square, or multi-row layout.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimg\n\n\nA tensor or numpy array representing a multi-channel image.\n\n\nax\nNoneType\nNone\nThe Matplotlib axis to use for plotting.\n\n\nfigsize\nNoneType\nNone\nThe size of the figure.\n\n\ntitle\nNoneType\nNone\nThe title of the image.\n\n\nmax_slices\nint\n3\nThe maximum number of slices to display.\n\n\nctx\nNoneType\nNone\nThe context to use for plotting.\n\n\nlayout\nstr\nhorizontal\nThe layout type: ‘horizontal’, ‘square’, or ‘multirow’.\n\n\nnum_cols\nint\n3\nThe number of columns for the ‘multirow’ layout. Ignored for other layouts.\n\n\ncmap\nNoneType\nNone\n\n\n\nnorm\nNoneType\nNone\n\n\n\naspect\nNoneType\nNone\n\n\n\ninterpolation\nNoneType\nNone\n\n\n\nalpha\nNoneType\nNone\n\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\n\n\n\nextent\nNoneType\nNone\n\n\n\ninterpolation_stage\nNoneType\nNone\n\n\n\nfilternorm\nbool\nTrue\n\n\n\nfilterrad\nfloat\n4.0\n\n\n\nresample\nNoneType\nNone\n\n\n\nurl\nNoneType\nNone\n\n\n\ndata\nNoneType\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nprint(data4D[35].shape)\nshow_multichannel(data4D[35], cmap='gray', layout='multirow', num_cols=1);\n\n(2, 256, 256)"
  },
  {
    "objectID": "visualize.html#display-3d-images",
    "href": "visualize.html#display-3d-images",
    "title": "Visualize",
    "section": "Display 3D images",
    "text": "Display 3D images\nFunction to display 3D images\n\nsource\n\nmosaic_image_3d\n\n mosaic_image_3d (t:(&lt;class'numpy.ndarray'&gt;,&lt;class'torch.Tensor'&gt;),\n                  axis:int=0, figsize:tuple=(15, 15), cmap:str='gray',\n                  nrow:int=10, alpha=1.0, return_grid=False,\n                  add_to_existing=False, **kwargs)\n\nPlots 2D slices of a 3D image alongside a prior specified axis.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nt\n(&lt;class ‘numpy.ndarray’&gt;, &lt;class ‘torch.Tensor’&gt;)\n\n3D image to plot\n\n\naxis\nint\n0\naxis to split 3D array to 2D images\n\n\nfigsize\ntuple\n(15, 15)\nsize of the figure\n\n\ncmap\nstr\ngray\ncolormap to use\n\n\nnrow\nint\n10\nnumber of images per row\n\n\nalpha\nfloat\n1.0\ntransparency of the image\n\n\nreturn_grid\nbool\nFalse\nreturn the grid for further processing\n\n\nadd_to_existing\nbool\nFalse\nadd to existing figure\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nshow_images_grid\n\n show_images_grid (images, ax=None, ncols=10, figsize=None, title=None,\n                   spacing=0.02, max_slices=3, ctx=None, cmap=None,\n                   norm=None, aspect=None, interpolation=None, alpha=None,\n                   vmin=None, vmax=None, origin=None, extent=None,\n                   interpolation_stage=None, filternorm=True,\n                   filterrad=4.0, resample=None, url=None, data=None,\n                   **kwargs)\n\n*Show a list of images arranged in a grid.\nReturns: - axes: matplotlib axes containing the grid of images.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nimages\n\n\nA list of images to display.\n\n\nax\nNoneType\nNone\nThe Matplotlib axis to use for plotting.\n\n\nncols\nint\n10\nThe number of columns in the grid.\n\n\nfigsize\nNoneType\nNone\nThe size of the figure.\n\n\ntitle\nNoneType\nNone\nThe title of the image.\n\n\nspacing\nfloat\n0.02\nThe spacing between subplots.\n\n\nmax_slices\nint\n3\nThe maximum number of slices to display.\n\n\nctx\nNoneType\nNone\nThe context to use for plotting.\n\n\ncmap\nNoneType\nNone\n\n\n\nnorm\nNoneType\nNone\n\n\n\naspect\nNoneType\nNone\n\n\n\ninterpolation\nNoneType\nNone\n\n\n\nalpha\nNoneType\nNone\n\n\n\nvmin\nNoneType\nNone\n\n\n\nvmax\nNoneType\nNone\n\n\n\norigin\nNoneType\nNone\n\n\n\nextent\nNoneType\nNone\n\n\n\ninterpolation_stage\nNoneType\nNone\n\n\n\nfilternorm\nbool\nTrue\n\n\n\nfilterrad\nfloat\n4.0\n\n\n\nresample\nNoneType\nNone\n\n\n\nurl\nNoneType\nNone\n\n\n\ndata\nNoneType\nNone\n\n\n\nkwargs\n\n\n\n\n\n\n\nmosaic_image_3d(torch_from_numpy(data), figsize=None)\n\n\n\n\n\nshow_images_grid(data, cmap='gray');"
  },
  {
    "objectID": "visualize.html#show-slices",
    "href": "visualize.html#show-slices",
    "title": "Visualize",
    "section": "Show slices",
    "text": "Show slices\n\nsource\n\nshow_plane\n\n show_plane (ax, plane, cmap='gray', title=None, lines=None,\n             linestyle='--', linecolor='white')\n\nDisplay a slice of the image tensor on a given axis with optional dashed lines.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nax\n\n\nThe axis object to display the slice on.\n\n\nplane\n\n\nA 2D numpy array representing the slice of the image tensor.\n\n\ncmap\nstr\ngray\nColormap to use for displaying the image.\n\n\ntitle\nNoneType\nNone\nTitle for the plot.\n\n\nlines\nNoneType\nNone\nA list of indices where dashed lines should be drawn on the plane.\n\n\nlinestyle\nstr\n–\nThe style of the dashed lines.\n\n\nlinecolor\nstr\nwhite\nThe color of the dashed lines.\n\n\n\n\nsource\n\n\nvisualize_slices\n\n visualize_slices (data, planes=None, showlines=True, **kwargs)\n\nVisualize slices of a 3D image tensor along its planes, rows, and columns.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nA 3D numpy array representing the image tensor.\n\n\nplanes\nNoneType\nNone\nA tuple containing the indices of the planes to visualize.\n\n\nshowlines\nbool\nTrue\nWhether to show dashed lines on the planes, rows, and columns.\n\n\nkwargs\n\n\n\n\n\n\n\nvisualize_slices(data, showlines=False)\n\n\n\n\n\nvisualize_slices(data, (25,100,150), linestyle=':')\n\n\n\n\n\nsource\n\n\nslice_explorer\n\n slice_explorer (data, order='CZYX', **kwargs)\n\nVisualizes the provided data using Plotly’s interactive imshow function with animation support.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nA 3D numpy array representing the image tensor.\n\n\norder\nstr\nCZYX\nThe order of dimensions in the data.\n\n\nkwargs\n\n\n\n\n\n\n\nslice_explorer(data4D, order='ZCYX', title='Cells 3D')\n\n\n                                                \n\n\n\n\n\nstatic image of slice explorer\n\n\n\nsource\n\n\nplot_volume\n\n plot_volume (values, opacity=0.1, min=0.1, max=0.8, surface_count=5,\n              width=800, height=600)\n\nInteractive visualization of a 3D volume using Plotly. The function assumes that ‘values’ is a 3D array representing the volume data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nvalues\n\n\nA 3D array of pixel values representing the volume.\n\n\nopacity\nfloat\n0.1\nOpacity level for the surfaces in the volume plot.\n\n\nmin\nfloat\n0.1\nMinimum threshold multiplier for the visualization.\n\n\nmax\nfloat\n0.8\nMaximum threshold multiplier for the visualization.\n\n\nsurface_count\nint\n5\nNumber of surfaces to display in the volume plot.\n\n\nwidth\nint\n800\nWidth of the plotted figure.\n\n\nheight\nint\n600\nHeight of the plotted figure.\n\n\n\n\nplot_volume(data[:, 50:150, 50:150])\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nstatic image of interactive 3D plot"
  },
  {
    "objectID": "Tutorials/tutorial_classification3d.html",
    "href": "Tutorials/tutorial_classification3d.html",
    "title": "Image Classification 3D",
    "section": "",
    "text": "Setup imports\n\nfrom bioMONAI.data import *\nfrom bioMONAI.transforms import *\nfrom bioMONAI.core import *\nfrom bioMONAI.core import Path\nfrom bioMONAI.losses import *\nfrom bioMONAI.metrics import *\nfrom bioMONAI.datasets import download_medmnist\nfrom bioMONAI.visualize import show_images_grid, mosaic_image_3d\nfrom bioMONAI.data import get_image_files\n\n\ndevice = get_device()\nprint(device)\n\ncuda\n\n\n\n\nDownload and store the dataset\n\ndata_flag = 'synapsemnist3d'\ndata_path = Path('../_data/medmnist_data/')\n\ninfo = download_medmnist(data_flag, data_path, download_only=True)\n\ndata_path = data_path/'synapsemnist3d'\ntrain_path = data_path/'train'\nval_path = data_path/'val'\ntest_path = data_path/'test'\n\nDataset 'synapsemnist3d' is already downloaded and available in '../_data/medmnist_data/synapsemnist3d'.\n\n\n\n\nCreate Dataloader\n\nbatch_size = 8\n\ndata = BioDataLoaders.class_from_folder(\n    data_path,\n    train='train',\n    valid='val',\n    vocab=info['label'],\n    item_tfms=[ScaleIntensity(), RandRot90(prob=0.5, spatial_axes=(1,2)), Resize(32)],\n    batch_tfms=None,\n    img_cls=BioImageStack,\n    bs=batch_size,\n    show_summary=True,\n    )\n\n# print length of training and validation datasets\nprint('train images:', len(data.train_ds.items), '\\nvalidation images:', len(data.valid_ds.items))\n\nSetting-up type transforms pipelines\nCollecting items from ../_data/medmnist_data/synapsemnist3d\nFound 1407 items\n2 datasets of sizes 1230,177\nSetting up Pipeline: BioImageBase.create -&gt; Tensor2BioImage -- {}\nSetting up Pipeline: parent_label -&gt; Categorize -- {'vocab': ['0', '1'], 'sort': True, 'add_na': False}\n\nBuilding one sample\n  Pipeline: BioImageBase.create -&gt; Tensor2BioImage -- {}\n    starting from\n      ../_data/medmnist_data/synapsemnist3d/train/1/train_865.tiff\n    applying BioImageBase.create gives\n      BioImageStack of size 1x28x28x28\n    applying Tensor2BioImage -- {} gives\n      BioImageStack of size 1x28x28x28\n  Pipeline: parent_label -&gt; Categorize -- {'vocab': ['0', '1'], 'sort': True, 'add_na': False}\n    starting from\n      ../_data/medmnist_data/synapsemnist3d/train/1/train_865.tiff\n    applying parent_label gives\n      1\n    applying Categorize -- {'vocab': ['0', '1'], 'sort': True, 'add_na': False} gives\n      TensorCategory(1)\n\nFinal sample: (BioImageStack([[[[0.8039, 0.8706, 0.4980,  ..., 0.4353, 0.1451, 0.2157],\n          [0.7882, 0.5529, 0.3922,  ..., 0.3216, 0.6118, 0.1529],\n          [0.5686, 0.7333, 0.8941,  ..., 0.1569, 0.8863, 0.2353],\n          ...,\n          [0.5608, 0.7255, 0.2941,  ..., 0.5255, 0.5686, 0.8392],\n          [0.1961, 0.2196, 0.2706,  ..., 0.4784, 0.7373, 0.4784],\n          [0.4784, 0.6353, 0.4118,  ..., 0.7882, 0.7255, 0.8078]],\n\n         [[0.5255, 0.4627, 0.7882,  ..., 0.2118, 0.4353, 0.3686],\n          [0.8235, 0.7804, 0.8980,  ..., 0.5765, 0.0039, 0.0784],\n          [0.9098, 0.8392, 0.6431,  ..., 0.0510, 0.7529, 0.1882],\n          ...,\n          [0.7098, 0.5294, 0.0941,  ..., 0.6039, 0.7255, 0.8784],\n          [0.5333, 0.2078, 0.6039,  ..., 0.4824, 0.7882, 0.7059],\n          [0.9137, 0.2627, 0.9882,  ..., 0.7765, 0.7843, 0.5765]],\n\n         [[0.6667, 0.7451, 0.7333,  ..., 0.8196, 0.5686, 0.7137],\n          [0.6314, 0.7059, 0.6549,  ..., 0.2667, 0.3569, 0.2902],\n          [0.6275, 0.5490, 0.6980,  ..., 0.3412, 0.0706, 0.1725],\n          ...,\n          [0.5843, 0.5725, 0.1961,  ..., 0.2667, 0.6431, 0.5255],\n          [0.6275, 0.3608, 0.3412,  ..., 0.5255, 0.6745, 0.7020],\n          [0.4275, 0.4235, 0.6588,  ..., 0.6275, 0.7098, 0.7294]],\n\n         ...,\n\n         [[0.6627, 0.6863, 0.8196,  ..., 0.3922, 0.5804, 0.9686],\n          [0.6667, 0.7294, 0.5529,  ..., 0.1882, 0.2157, 0.0549],\n          [0.7490, 0.8078, 0.6588,  ..., 0.7529, 0.7294, 0.1333],\n          ...,\n          [0.7216, 0.5804, 0.5961,  ..., 0.4196, 0.1725, 0.8588],\n          [0.5059, 0.5569, 0.6549,  ..., 0.7804, 0.4549, 0.5098],\n          [0.5725, 0.6118, 0.5608,  ..., 0.7373, 0.8275, 0.2549]],\n\n         [[0.6510, 0.8902, 0.6471,  ..., 0.4392, 0.4275, 0.5529],\n          [0.6392, 0.6784, 0.7294,  ..., 0.1255, 0.2275, 0.5412],\n          [0.6314, 0.5961, 0.7176,  ..., 0.3020, 0.2000, 0.5765],\n          ...,\n          [0.4157, 0.5451, 0.4980,  ..., 0.2392, 0.3451, 0.3529],\n          [0.7725, 0.6353, 0.7255,  ..., 0.9294, 0.4353, 0.3020],\n          [0.6314, 0.8039, 0.6353,  ..., 0.7569, 0.8353, 0.2706]],\n\n         [[0.6627, 0.6667, 0.5765,  ..., 0.7608, 0.8392, 0.5333],\n          [0.6863, 0.4549, 0.7490,  ..., 0.3451, 0.7608, 0.7765],\n          [0.6275, 0.6510, 0.5412,  ..., 0.3804, 0.4157, 0.8784],\n          ...,\n          [0.5686, 0.4745, 0.6157,  ..., 0.6863, 0.0902, 0.4863],\n          [0.5059, 0.5569, 0.6824,  ..., 0.6471, 0.6431, 0.2196],\n          [0.4824, 0.7176, 0.7569,  ..., 0.8196, 0.8235, 0.4275]]]]), TensorCategory(1))\n\n\nCollecting items from ../_data/medmnist_data/synapsemnist3d\nFound 1407 items\n2 datasets of sizes 1230,177\nSetting up Pipeline: BioImageBase.create -&gt; Tensor2BioImage -- {}\nSetting up Pipeline: parent_label -&gt; Categorize -- {'vocab': ['0', '1'], 'sort': True, 'add_na': False}\nSetting up after_item: Pipeline: ScaleIntensity -&gt; Resize -&gt; RandRot90 -- {'prob': 0.5, 'max_k': 3, 'spatial_axes': (1, 2), 'ndim': 2, 'lazy': False, 'p': 1.0} -&gt; ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: \n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: ScaleIntensity -&gt; Resize -&gt; RandRot90 -- {'prob': 0.5, 'max_k': 3, 'spatial_axes': (1, 2), 'ndim': 2, 'lazy': False, 'p': 1.0} -&gt; ToTensor\n    starting from\n      (BioImageStack of size 1x28x28x28, TensorCategory(1))\n    applying ScaleIntensity gives\n      (BioImageStack of size 1x28x28x28, TensorCategory(1))\n    applying Resize gives\n      (BioImageStack of size 1x32x32x32, TensorCategory(1))\n    applying RandRot90 -- {'prob': 0.5, 'max_k': 3, 'spatial_axes': (1, 2), 'ndim': 2, 'lazy': False, 'p': 1.0} gives\n      (BioImageStack of size 1x32x32x32, TensorCategory(1))\n    applying ToTensor gives\n      (BioImageStack of size 1x32x32x32, TensorCategory(1))\n\nAdding the next 15 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nNo batch_tfms to apply\nNone\ntrain images: 1230 \nvalidation images: 177\n\n\n\ndata.show_batch()\n\n\n\n\n\n\nLoad and train a 3D model\n\nfrom monai.networks.nets import DenseNet169, SEResNet50\nfrom fastai.vision.all import BalancedAccuracy, CrossEntropyLossFlat, Learner\n# from torch.nn import CrossEntropyLoss\n\n# model = DenseNet169(spatial_dims=3, in_channels=1, out_channels=2)\nmodel = SEResNet50(spatial_dims=3, in_channels=1, num_classes=2)\n\nloss = CrossEntropyLossFlat()\nmetric = BalancedAccuracy()\n\ntrainer = fastTrainer(data, model, loss_fn=loss, metrics=metric, show_summary=True, find_lr=True)\n\n\n\n\n\n\n\n\nSEResNet50 (Input shape: 16 x 1 x 32 x 32 x 32)\n============================================================================\nLayer (type)         Output Shape         Param #    Trainable \n============================================================================\n                     16 x 64 x 16 x 16 x \nConv3d                                    21952      True      \nBatchNorm3d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 64 x 8 x 8 x 8 \nMaxPool3d                                                      \nConv3d                                    4096       True      \nBatchNorm3d                               128        True      \nReLU                                                           \nConv3d                                    110592     True      \nBatchNorm3d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 8 x 8 x  \nConv3d                                    16384      True      \nBatchNorm3d                               512        True      \n____________________________________________________________________________\n                     16 x 256 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 16             \nLinear                                    4112       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256            \nLinear                                    4352       True      \nSigmoid                                                        \n____________________________________________________________________________\n                     16 x 256 x 8 x 8 x  \nConv3d                                    16384      True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 64 x 8 x 8 x 8 \nConv3d                                    16384      True      \nBatchNorm3d                               128        True      \nReLU                                                           \nConv3d                                    110592     True      \nBatchNorm3d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 8 x 8 x  \nConv3d                                    16384      True      \nBatchNorm3d                               512        True      \n____________________________________________________________________________\n                     16 x 256 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 16             \nLinear                                    4112       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256            \nLinear                                    4352       True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 64 x 8 x 8 x 8 \nConv3d                                    16384      True      \nBatchNorm3d                               128        True      \nReLU                                                           \nConv3d                                    110592     True      \nBatchNorm3d                               128        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 8 x 8 x  \nConv3d                                    16384      True      \nBatchNorm3d                               512        True      \n____________________________________________________________________________\n                     16 x 256 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 16             \nLinear                                    4112       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256            \nLinear                                    4352       True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 128 x 4 x 4 x  \nConv3d                                    32768      True      \nBatchNorm3d                               256        True      \nReLU                                                           \nConv3d                                    442368     True      \nBatchNorm3d                               256        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512 x 4 x 4 x  \nConv3d                                    65536      True      \nBatchNorm3d                               1024       True      \n____________________________________________________________________________\n                     16 x 512 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 32             \nLinear                                    16416      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512            \nLinear                                    16896      True      \nSigmoid                                                        \n____________________________________________________________________________\n                     16 x 512 x 4 x 4 x  \nConv3d                                    131072     True      \nBatchNorm3d                               1024       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 128 x 4 x 4 x  \nConv3d                                    65536      True      \nBatchNorm3d                               256        True      \nReLU                                                           \nConv3d                                    442368     True      \nBatchNorm3d                               256        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512 x 4 x 4 x  \nConv3d                                    65536      True      \nBatchNorm3d                               1024       True      \n____________________________________________________________________________\n                     16 x 512 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 32             \nLinear                                    16416      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512            \nLinear                                    16896      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 128 x 4 x 4 x  \nConv3d                                    65536      True      \nBatchNorm3d                               256        True      \nReLU                                                           \nConv3d                                    442368     True      \nBatchNorm3d                               256        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512 x 4 x 4 x  \nConv3d                                    65536      True      \nBatchNorm3d                               1024       True      \n____________________________________________________________________________\n                     16 x 512 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 32             \nLinear                                    16416      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512            \nLinear                                    16896      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 128 x 4 x 4 x  \nConv3d                                    65536      True      \nBatchNorm3d                               256        True      \nReLU                                                           \nConv3d                                    442368     True      \nBatchNorm3d                               256        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512 x 4 x 4 x  \nConv3d                                    65536      True      \nBatchNorm3d                               1024       True      \n____________________________________________________________________________\n                     16 x 512 x 1 x 1 x  \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 32             \nLinear                                    16416      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512            \nLinear                                    16896      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 2 x 2 x  \nConv3d                                    131072     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024 x 2 x 2 x \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     16 x 1024 x 1 x 1 x \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 64             \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024           \nLinear                                    66560      True      \nSigmoid                                                        \n____________________________________________________________________________\n                     16 x 1024 x 2 x 2 x \nConv3d                                    524288     True      \nBatchNorm3d                               2048       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024 x 2 x 2 x \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     16 x 1024 x 1 x 1 x \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 64             \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024           \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024 x 2 x 2 x \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     16 x 1024 x 1 x 1 x \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 64             \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024           \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024 x 2 x 2 x \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     16 x 1024 x 1 x 1 x \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 64             \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024           \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024 x 2 x 2 x \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     16 x 1024 x 1 x 1 x \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 64             \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024           \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 256 x 2 x 2 x  \nConv3d                                    262144     True      \nBatchNorm3d                               512        True      \nReLU                                                           \nConv3d                                    1769472    True      \nBatchNorm3d                               512        True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024 x 2 x 2 x \nConv3d                                    262144     True      \nBatchNorm3d                               2048       True      \n____________________________________________________________________________\n                     16 x 1024 x 1 x 1 x \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 64             \nLinear                                    65600      True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 1024           \nLinear                                    66560      True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512 x 1 x 1 x  \nConv3d                                    524288     True      \nBatchNorm3d                               1024       True      \nReLU                                                           \nConv3d                                    7077888    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 2048 x 1 x 1 x \nConv3d                                    1048576    True      \nBatchNorm3d                               4096       True      \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 128            \nLinear                                    262272     True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 2048           \nLinear                                    264192     True      \nSigmoid                                                        \n____________________________________________________________________________\n                     16 x 2048 x 1 x 1 x \nConv3d                                    2097152    True      \nBatchNorm3d                               4096       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512 x 1 x 1 x  \nConv3d                                    1048576    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \nConv3d                                    7077888    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 2048 x 1 x 1 x \nConv3d                                    1048576    True      \nBatchNorm3d                               4096       True      \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 128            \nLinear                                    262272     True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 2048           \nLinear                                    264192     True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \n____________________________________________________________________________\n                     16 x 512 x 1 x 1 x  \nConv3d                                    1048576    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \nConv3d                                    7077888    True      \nBatchNorm3d                               1024       True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 2048 x 1 x 1 x \nConv3d                                    1048576    True      \nBatchNorm3d                               4096       True      \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 128            \nLinear                                    262272     True      \nReLU                                                           \n____________________________________________________________________________\n                     16 x 2048           \nLinear                                    264192     True      \nSigmoid                                                        \nIdentity                                                       \nReLU                                                           \nAdaptiveAvgPool3d                                              \n____________________________________________________________________________\n                     16 x 2              \nLinear                                    4098       True      \n____________________________________________________________________________\n\nTotal params: 48,690,162\nTotal trainable params: 48,690,162\nTotal non-trainable params: 0\n\nOptimizer used: &lt;function Adam&gt;\nLoss function: FlattenedLoss of CrossEntropyLoss()\n\nCallbacks:\n  - TrainEvalCallback\n  - CastToTensor\n  - Recorder\n  - ProgressCallback\n  - ShowGraphCallback\nInferred learning rate:  3e-05\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrainer.fit(20)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nbalanced_accuracy_score\ntime\n\n\n\n\n0\n0.635229\n0.621256\n0.480862\n00:03\n\n\n1\n0.551267\n0.631951\n0.493944\n00:03\n\n\n2\n0.503740\n0.617897\n0.543120\n00:03\n\n\n3\n0.455072\n0.631129\n0.538033\n00:03\n\n\n4\n0.431959\n0.676425\n0.548692\n00:03\n\n\n5\n0.410487\n0.708722\n0.552810\n00:03\n\n\n6\n0.383551\n0.726638\n0.556686\n00:03\n\n\n7\n0.350918\n0.741261\n0.574855\n00:03\n\n\n8\n0.318454\n0.719016\n0.562016\n00:03\n\n\n9\n0.292503\n0.710118\n0.588905\n00:03\n\n\n10\n0.282804\n0.766639\n0.565649\n00:03\n\n\n11\n0.265645\n0.827757\n0.584060\n00:03\n\n\n12\n0.234502\n0.891670\n0.594477\n00:03\n\n\n13\n0.236331\n0.797980\n0.570736\n00:03\n\n\n14\n0.231850\n0.785105\n0.587694\n00:03\n\n\n15\n0.221538\n0.824789\n0.561773\n00:03\n\n\n16\n0.211560\n0.820286\n0.603440\n00:03\n\n\n17\n0.191282\n0.809323\n0.645349\n00:03\n\n\n18\n0.197415\n0.938505\n0.608769\n00:03\n\n\n19\n0.163146\n0.835267\n0.630814\n00:03\n\n\n\n\n\n\n\n\n\n#trainer.show_results(cmap='gray')\n\n\n# trainer.save('tmp-model')\n\n\n\nTest data\nEvaluate the performance of the selected model on unseen data. It’s important to not touch this data until you have fine tuned your model to get an unbiased evaluation!\n\ntest_data = data.test_dl(get_image_files(test_path), with_labels=True)\n# print length of test dataset\nprint('test images:', len(test_data))\n\ntest images: 22\n\n\n\nevaluate_classification_model(trainer, test_data, show_graph=False, show_results=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.45      0.25      0.32        95\n           1       0.76      0.89      0.82       257\n\n    accuracy                           0.72       352\n   macro avg       0.61      0.57      0.57       352\nweighted avg       0.68      0.72      0.69       352\n\n\nMost Confused Classes:\n[('0', '1', 71), ('1', '0', 29)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCrossEntropyLossFlat\n\n\n\n\n\nMean\n0.579085\n\n\nMedian\n0.365717\n\n\nStandard Deviation\n0.352170\n\n\nMin\n0.313332\n\n\nMax\n1.312281\n\n\nQ1\n0.321334\n\n\nQ3\n0.799577\n\n\n\n\n\n\n\n{'CrossEntropyLossFlat': [0.774454653263092,\n  0.318200021982193,\n  0.33017563819885254,\n  0.41057121753692627,\n  0.31440508365631104,\n  0.844761073589325,\n  0.3220670819282532,\n  0.48011723160743713,\n  0.34339386224746704,\n  0.7992609143257141,\n  0.3209790885448456,\n  0.3213556110858917,\n  0.3668822646141052,\n  0.31483641266822815,\n  0.33703890442848206,\n  0.315140038728714,\n  0.3237593173980713,\n  0.3263738453388214,\n  0.316360741853714,\n  0.3779691457748413,\n  0.3395267128944397,\n  0.471957802772522,\n  0.3599473834037781,\n  0.3186264932155609,\n  0.4891008734703064,\n  0.31581777334213257,\n  0.34245723485946655,\n  0.3148588538169861,\n  0.3142279386520386,\n  0.3137844204902649,\n  0.3407739996910095,\n  0.3138473927974701,\n  1.2550413608551025,\n  0.6138912439346313,\n  0.319322407245636,\n  0.36110880970954895,\n  1.2329554557800293,\n  0.3185996413230896,\n  0.3134235739707947,\n  0.32176312804222107,\n  0.9185994863510132,\n  0.3147069215774536,\n  0.3383767306804657,\n  0.33764490485191345,\n  0.766112208366394,\n  0.31380966305732727,\n  0.3330734968185425,\n  0.3716140687465668,\n  0.31887057423591614,\n  0.5189138650894165,\n  0.32133209705352783,\n  0.3347298204898834,\n  0.33826369047164917,\n  1.1603962182998657,\n  0.3321700096130371,\n  0.40761396288871765,\n  0.37806832790374756,\n  0.3171963393688202,\n  0.3247448801994324,\n  0.3339906930923462,\n  0.3236417770385742,\n  0.8531150221824646,\n  0.4010605812072754,\n  0.6677944660186768,\n  0.31819048523902893,\n  0.31961238384246826,\n  0.3170432150363922,\n  0.320722371339798,\n  0.31537574529647827,\n  0.7657636404037476,\n  0.3147180676460266,\n  0.3774318993091583,\n  0.32424333691596985,\n  0.3194916546344757,\n  0.31439343094825745,\n  0.3137437403202057,\n  0.31807634234428406,\n  0.35780420899391174,\n  0.31612110137939453,\n  0.3468569219112396,\n  0.3165302276611328,\n  1.2604306936264038,\n  0.6072260737419128,\n  0.3196876347064972,\n  0.5903136134147644,\n  0.32133400440216064,\n  0.3335550129413605,\n  0.3169865906238556,\n  0.32613039016723633,\n  0.31817173957824707,\n  0.3178180456161499,\n  0.7030856609344482,\n  0.4113439619541168,\n  0.513680100440979,\n  0.7171344757080078,\n  0.6425323486328125,\n  0.49993032217025757,\n  0.902450442314148,\n  0.5388505458831787,\n  0.32013943791389465,\n  0.365762859582901,\n  0.3219187557697296,\n  0.3162975013256073,\n  0.37058165669441223,\n  0.31374529004096985,\n  0.3143838346004486,\n  0.31940218806266785,\n  0.42742276191711426,\n  0.31777265667915344,\n  0.37781262397766113,\n  0.3284843862056732,\n  0.31790071725845337,\n  0.3146040737628937,\n  0.3316923677921295,\n  0.3260699212551117,\n  0.8397183418273926,\n  0.4270147383213043,\n  0.31591224670410156,\n  0.3329673707485199,\n  0.4415510296821594,\n  0.3326556086540222,\n  0.3403041660785675,\n  0.31946948170661926,\n  0.3143164813518524,\n  0.3163732588291168,\n  0.5737225413322449,\n  1.2300416231155396,\n  0.35673272609710693,\n  0.5840599536895752,\n  0.3299773335456848,\n  0.31349238753318787,\n  0.9193084836006165,\n  0.5920998454093933,\n  0.5974476337432861,\n  0.34451645612716675,\n  0.3217872381210327,\n  0.33784669637680054,\n  0.3506168723106384,\n  0.38133949041366577,\n  0.3133322298526764,\n  0.3662223517894745,\n  0.3203936219215393,\n  0.32202932238578796,\n  0.8448151350021362,\n  0.3384424149990082,\n  0.3409174382686615,\n  0.3279676139354706,\n  0.32318514585494995,\n  0.3555371165275574,\n  0.3227797746658325,\n  0.3136154115200043,\n  0.3809106647968292,\n  1.239356279373169,\n  0.3147037923336029,\n  0.37222132086753845,\n  0.31389474868774414,\n  0.3232269883155823,\n  0.3201051652431488,\n  0.4066437780857086,\n  0.49863654375076294,\n  0.3317236602306366,\n  0.3358179032802582,\n  0.35092276334762573,\n  0.6856046915054321,\n  0.326602965593338,\n  0.3135267198085785,\n  0.5715864896774292,\n  0.3762386441230774,\n  0.3296234905719757,\n  0.3209594488143921,\n  0.31375357508659363,\n  0.5595866441726685,\n  0.534756600856781,\n  0.3545079529285431,\n  0.3197646141052246,\n  0.5882450342178345,\n  0.4140385091304779,\n  0.31983232498168945,\n  0.31506410241127014,\n  1.0814259052276611,\n  0.3185274302959442,\n  0.358377069234848,\n  0.3467337191104889,\n  0.32226091623306274,\n  0.315092533826828,\n  0.3153539001941681,\n  1.152876377105713,\n  0.3367074728012085,\n  0.3201521635055542,\n  0.3218955993652344,\n  0.3144581913948059,\n  0.3863433301448822,\n  0.37176984548568726,\n  0.3274468183517456,\n  0.5723298192024231,\n  0.31715962290763855,\n  0.3421732187271118,\n  0.31711438298225403,\n  0.43717098236083984,\n  0.3309427499771118,\n  1.1454029083251953,\n  0.3358016014099121,\n  0.31929147243499756,\n  0.3138100206851959,\n  0.6117279529571533,\n  1.104782223701477,\n  0.31349360942840576,\n  0.3179551064968109,\n  0.3160557448863983,\n  0.35090741515159607,\n  0.3288147747516632,\n  0.3341538906097412,\n  0.31420233845710754,\n  0.3179135322570801,\n  0.46978577971458435,\n  0.354347288608551,\n  0.46689116954803467,\n  0.31368517875671387,\n  0.4355512857437134,\n  0.3840929865837097,\n  0.31540313363075256,\n  0.3159570097923279,\n  0.31910523772239685,\n  0.3296414911746979,\n  1.1947180032730103,\n  0.35264918208122253,\n  0.31755489110946655,\n  0.924910843372345,\n  0.38010725378990173,\n  0.8005253076553345,\n  0.7527629137039185,\n  0.3148420751094818,\n  0.4456721246242523,\n  0.3220488429069519,\n  0.3546367287635803,\n  0.32615309953689575,\n  1.1956816911697388,\n  0.3218413293361664,\n  0.3442233204841614,\n  1.1246941089630127,\n  0.31488722562789917,\n  0.325131893157959,\n  0.3198016583919525,\n  0.6594740748405457,\n  0.32243356108665466,\n  0.4131261110305786,\n  0.31342607736587524,\n  0.6393607258796692,\n  0.31705692410469055,\n  0.3182063400745392,\n  0.3656718134880066,\n  0.34574732184410095,\n  0.4350852072238922,\n  0.32929643988609314,\n  0.33526262640953064,\n  0.31507402658462524,\n  0.38317057490348816,\n  1.1917997598648071,\n  1.2859504222869873,\n  0.5727535486221313,\n  0.3963329493999481,\n  1.2985225915908813,\n  1.1050751209259033,\n  0.3291212320327759,\n  1.2858319282531738,\n  1.2833502292633057,\n  0.42606523633003235,\n  1.291544795036316,\n  1.2854942083358765,\n  1.163907527923584,\n  0.7625617980957031,\n  0.3328608274459839,\n  1.1553915739059448,\n  1.2894128561019897,\n  1.1384624242782593,\n  0.6427332162857056,\n  0.33810117840766907,\n  1.2038732767105103,\n  1.2018780708312988,\n  1.3115532398223877,\n  1.2713592052459717,\n  0.9791324138641357,\n  0.9138946533203125,\n  0.732268750667572,\n  1.3005484342575073,\n  1.2619683742523193,\n  1.118849277496338,\n  1.300262689590454,\n  1.2907772064208984,\n  0.6142510771751404,\n  0.43418967723846436,\n  0.42671751976013184,\n  1.3103286027908325,\n  1.272247076034546,\n  1.1238124370574951,\n  0.8394005298614502,\n  1.2415887117385864,\n  0.6447073221206665,\n  1.0533088445663452,\n  0.4442345201969147,\n  0.8188450336456299,\n  1.1442989110946655,\n  1.03316068649292,\n  0.9480876326560974,\n  1.2924745082855225,\n  1.302399754524231,\n  0.3291172683238983,\n  0.3395111858844757,\n  1.0655930042266846,\n  1.285320520401001,\n  0.9017131328582764,\n  1.3030693531036377,\n  0.3195391297340393,\n  1.2773923873901367,\n  0.41966715455055237,\n  1.1800206899642944,\n  0.6481932997703552,\n  1.312171220779419,\n  0.8820117712020874,\n  0.37759438157081604,\n  1.234088659286499,\n  1.301080584526062,\n  1.178640365600586,\n  1.1812586784362793,\n  1.3024195432662964,\n  1.310179352760315,\n  0.3236105442047119,\n  0.5301274657249451,\n  0.7019070386886597,\n  0.9223760366439819,\n  0.3360963761806488,\n  1.2860946655273438,\n  0.6988599300384521,\n  1.3122812509536743,\n  0.40604057908058167,\n  0.9417935013771057,\n  0.9695171117782593,\n  0.8945355415344238,\n  1.2793376445770264,\n  1.2844369411468506,\n  0.9603939056396484,\n  1.159122109413147,\n  1.2224607467651367,\n  1.3122665882110596,\n  1.1590839624404907,\n  1.2659696340560913,\n  1.3027695417404175,\n  1.2399157285690308,\n  0.6415858268737793,\n  0.46775275468826294,\n  1.1887423992156982,\n  0.7260570526123047]}"
  },
  {
    "objectID": "Tutorials/tutorial_denoising.html",
    "href": "Tutorials/tutorial_denoising.html",
    "title": "Denoising",
    "section": "",
    "text": "from bioMONAI.data import *\nfrom bioMONAI.transforms import *\nfrom bioMONAI.core import *\nfrom bioMONAI.core import Path\nfrom bioMONAI.data import get_image_files, get_target, RandomSplitter\nfrom bioMONAI.losses import *\nfrom bioMONAI.losses import SSIMLoss\nfrom bioMONAI.metrics import *\nfrom bioMONAI.datasets import download_file\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndevice = get_device()\nprint(device)\n\ncuda\n\n\n\nDownload Data\n\n# Specify the directory where you want to save the downloaded files\noutput_directory = \"../_data/U2OS\"\n# Define the base URL for the dataset\nurl = 'http://csbdeep.bioimagecomputing.com/example_data/snr_7_binning_2.zip'\n\n# Download only the first two images\ndownload_file(url, output_directory, extract=True)\n\nThe file has been downloaded and saved to: ../_data/U2OS\nDecompression (if needed) has been handled automatically.\n\n\n\n\nCreate Dataloader\n\nX_path = '../_data/U2OS/128a57f165e1044e34d9a6ef46e66b3c-snr_7_binning_2.zip.unzip/train/low/'\n\nbs = 32\npatch_size = 96\n\nitemTfms = [RandCropND(patch_size), RandRot90(prob=.75), RandFlip(prob=0.75)]\nbatchTfms = [ScaleIntensityPercentiles()]\n\nget_target_fn = get_target('GT', same_filename=True, relative_path=True)\n\ndata = BioDataLoaders.from_folder(\n    X_path, \n    get_target_fn, \n    valid_pct=0.05, \n    seed=42, \n    item_tfms=itemTfms,\n    batch_tfms=batchTfms, \n    show_summary=False,\n    bs = bs,\n    )\n\n# print length of training and validation datasets\nprint('train images:', len(data.train_ds.items), '\\nvalidation images:', len(data.valid_ds.items))\n\ntrain images: 2335 \nvalidation images: 122\n\n\n\ndata.show_batch(cmap='magma')\n\n\n\n\n\n\nLoad and train a 2D model\n\nfrom bioMONAI.nets import create_unet_model, resnet34\n\nmodel = create_unet_model(resnet34, 1, (128,128), True, n_in=1, cut=7)\n\n\nloss = CombinedLoss(mse_weight=0.8, mae_weight=0.1)\n\nmetrics = [MSEMetric(), MAEMetric(), SSIMMetric(2)]\n\ntrainer = fastTrainer(data, model, loss_fn=loss, metrics=metrics, show_summary=False)\n\n\ntrainer.fine_tune(50, freeze_epochs=2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nMSE\nMAE\nSSIM\ntime\n\n\n\n\n0\n0.059263\n0.031750\n0.003062\n0.030812\n0.737810\n00:05\n\n\n1\n0.031509\n0.021351\n0.002592\n0.025890\n0.833123\n00:04\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nMSE\nMAE\nSSIM\ntime\n\n\n\n\n0\n0.018390\n0.017519\n0.001561\n0.020792\n0.858087\n00:04\n\n\n1\n0.016879\n0.015520\n0.001512\n0.020095\n0.876995\n00:04\n\n\n2\n0.015920\n0.016230\n0.001825\n0.021673\n0.873977\n00:04\n\n\n3\n0.015772\n0.015743\n0.001493\n0.019981\n0.874495\n00:04\n\n\n4\n0.015445\n0.018726\n0.002763\n0.026642\n0.861489\n00:04\n\n\n5\n0.015966\n0.015048\n0.001492\n0.019624\n0.881081\n00:04\n\n\n6\n0.016098\n0.016459\n0.001995\n0.022954\n0.874330\n00:04\n\n\n7\n0.015651\n0.015040\n0.001728\n0.020668\n0.884089\n00:04\n\n\n8\n0.015609\n0.016344\n0.002146\n0.022793\n0.876518\n00:04\n\n\n9\n0.015790\n0.016388\n0.001928\n0.021907\n0.873452\n00:04\n\n\n10\n0.016008\n0.016197\n0.001503\n0.020361\n0.870417\n00:05\n\n\n11\n0.015512\n0.015503\n0.001408\n0.019778\n0.876012\n00:05\n\n\n12\n0.015767\n0.015337\n0.001368\n0.019256\n0.876830\n00:05\n\n\n13\n0.015257\n0.014446\n0.001362\n0.019063\n0.885507\n00:04\n\n\n14\n0.015756\n0.014684\n0.001553\n0.020141\n0.885727\n00:04\n\n\n15\n0.015227\n0.015024\n0.001379\n0.019199\n0.879995\n00:04\n\n\n16\n0.015261\n0.014982\n0.001360\n0.019180\n0.880241\n00:04\n\n\n17\n0.014247\n0.014851\n0.002221\n0.022847\n0.892102\n00:04\n\n\n18\n0.014091\n0.014845\n0.001407\n0.019615\n0.882418\n00:04\n\n\n19\n0.014043\n0.013648\n0.001350\n0.018536\n0.892850\n00:04\n\n\n20\n0.013710\n0.014114\n0.001372\n0.018851\n0.888687\n00:04\n\n\n21\n0.013437\n0.016465\n0.001930\n0.022347\n0.873133\n00:04\n\n\n22\n0.013622\n0.014885\n0.001425\n0.019470\n0.882027\n00:04\n\n\n23\n0.013829\n0.014458\n0.001389\n0.019248\n0.885781\n00:04\n\n\n24\n0.014047\n0.013161\n0.001478\n0.018917\n0.899137\n00:04\n\n\n25\n0.013911\n0.012902\n0.001281\n0.018061\n0.899292\n00:04\n\n\n26\n0.013629\n0.013000\n0.001469\n0.018982\n0.900730\n00:04\n\n\n27\n0.013198\n0.014055\n0.001335\n0.018980\n0.889103\n00:04\n\n\n28\n0.013180\n0.012791\n0.001315\n0.018064\n0.900674\n00:04\n\n\n29\n0.013422\n0.012379\n0.001298\n0.017815\n0.904412\n00:04\n\n\n30\n0.013412\n0.013191\n0.001374\n0.018449\n0.897529\n00:04\n\n\n31\n0.013132\n0.012420\n0.001276\n0.017726\n0.903737\n00:04\n\n\n32\n0.012796\n0.012543\n0.001240\n0.017730\n0.902221\n00:04\n\n\n33\n0.012817\n0.012291\n0.001316\n0.018162\n0.905779\n00:04\n\n\n34\n0.012658\n0.012130\n0.001254\n0.017662\n0.906392\n00:04\n\n\n35\n0.012476\n0.012438\n0.001278\n0.018083\n0.903923\n00:04\n\n\n36\n0.012751\n0.012415\n0.001341\n0.018224\n0.904799\n00:04\n\n\n37\n0.012631\n0.014831\n0.001759\n0.020762\n0.886519\n00:04\n\n\n38\n0.012579\n0.012348\n0.001227\n0.017694\n0.904028\n00:04\n\n\n39\n0.012365\n0.012506\n0.001264\n0.017780\n0.902831\n00:04\n\n\n40\n0.012290\n0.012359\n0.001237\n0.017511\n0.903814\n00:04\n\n\n41\n0.012234\n0.011951\n0.001238\n0.017431\n0.907826\n00:04\n\n\n42\n0.012528\n0.012128\n0.001226\n0.017376\n0.905901\n00:04\n\n\n43\n0.012046\n0.011911\n0.001287\n0.017585\n0.908764\n00:05\n\n\n44\n0.012103\n0.012053\n0.001222\n0.017326\n0.906574\n00:04\n\n\n45\n0.012201\n0.011962\n0.001235\n0.017346\n0.907602\n00:04\n\n\n46\n0.012174\n0.011858\n0.001234\n0.017341\n0.908638\n00:04\n\n\n47\n0.011766\n0.013627\n0.001352\n0.018609\n0.893159\n00:04\n\n\n48\n0.011853\n0.012117\n0.001251\n0.017631\n0.906461\n00:04\n\n\n49\n0.011875\n0.012058\n0.001255\n0.017596\n0.907052\n00:04\n\n\n\n\n\n\n\n\n\ntrainer.show_results(cmap='magma')\n\n\n\n\n\n\n\n\n\n\n\n\n# trainer.save('tmp-model')\n\n\n\nTest data\nEvaluate the performance of the selected model on unseen data. It’s important to not touch this data until you have fine tuned your model to get an unbiased evaluation!\n\ntest_X_path = '../_data/U2OS/128a57f165e1044e34d9a6ef46e66b3c-snr_7_binning_2.zip.unzip/test/low/'\n\ntest_data = data.test_dl(get_image_files(test_X_path), with_labels=True)\n\nevaluate_model(trainer, test_data, metrics=SSIMMetric(2));\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCombinedLoss\n\n\n\n\n\nMean\n0.016833\n\n\nMedian\n0.013806\n\n\nStandard Deviation\n0.012330\n\n\nMin\n0.000871\n\n\nMax\n0.094672\n\n\nQ1\n0.010612\n\n\nQ3\n0.018312\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nSSIM\n\n\n\n\n\nMean\n0.860965\n\n\nMedian\n0.893404\n\n\nStandard Deviation\n0.118673\n\n\nMin\n0.104494\n\n\nMax\n0.992975\n\n\nQ1\n0.852688\n\n\nQ3\n0.918993"
  },
  {
    "objectID": "metrics.html",
    "href": "metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "source\n\nPSNRMetric\n\n PSNRMetric (max_val, **kwargs)\n\n\nsource\n\n\nRMSEMetric\n\n RMSEMetric (**kwargs)\n\n\nsource\n\n\nMAEMetric\n\n MAEMetric (**kwargs)\n\n\nsource\n\n\nMSEMetric\n\n MSEMetric (**kwargs)\n\n\nsource\n\n\nSSIMMetric\n\n SSIMMetric (spatial_dims=3, **kwargs)\n\n\n\nFourier Ring Correlation\n\nRadial mask\n\nsource\n\n\n\nradial_mask\n\n radial_mask (r, cx=128, cy=128, sx=256, sy=256, delta=1)\n\n*Generate a radial mask.\nReturns: - numpy.ndarray: Radial mask.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nr\n\n\nRadius of the radial mask\n\n\ncx\nint\n128\nX coordinate mask center\n\n\ncy\nint\n128\nY coordinate maske center\n\n\nsx\nint\n256\nSize of the x-axis\n\n\nsy\nint\n256\nSize of the y-axis\n\n\ndelta\nint\n1\nThickness adjustment for the circular mask\n\n\n\n\nsource\n\n\nget_radial_masks\n\n get_radial_masks (width, height)\n\n*Generates a set of radial masks and corresponding to spatial frequencies.\nReturns: tuple: A tuple containing: - numpy.ndarray: Array of radial masks. - numpy.ndarray: Array of spatial frequencies corresponding to the masks.*\n\n\n\n\nDetails\n\n\n\n\nwidth\nWidth of the image\n\n\nheight\nHeight of the image\n\n\n\n\nFourier ring correlation\n\nsource\n\n\n\nget_fourier_ring_correlations\n\n get_fourier_ring_correlations (image1, image2)\n\n*Compute Fourier Ring Correlation (FRC) between two images.\nReturns: tuple: A tuple containing: - torch.Tensor: Fourier Ring Correlation values. - torch.Tensor: Array of spatial frequencies.*\n\n\n\n\nDetails\n\n\n\n\nimage1\nFirst input image\n\n\nimage2\nSecond input image\n\n\n\n\nsource\n\n\nFRCMetric\n\n FRCMetric (image1, image2)\n\n*Compute the area under the Fourier Ring Correlation (FRC) curve between two images.\nReturns: - float: The area under the FRC curve.*\n\n\n\n\nDetails\n\n\n\n\nimage1\nFirst input image\n\n\nimage2\nSecond input image"
  },
  {
    "objectID": "losses.html",
    "href": "losses.html",
    "title": "Loss functions",
    "section": "",
    "text": "source\n\nMSELoss\n\n MSELoss (inp:Any, targ:Any)\n\n\nsource\n\n\nL1Loss\n\n L1Loss (inp:Any, targ:Any)\n\n\n\n\nSSIMLoss\n\n SSIMLoss (spatial_dims:int, data_range:float=1.0,\n           kernel_type:monai.metrics.regression.KernelType|str=gaussian,\n           win_size:int|collections.abc.Sequence[int]=11,\n           kernel_sigma:float|collections.abc.Sequence[float]=1.5,\n           k1:float=0.01, k2:float=0.03,\n           reduction:monai.utils.enums.LossReduction|str=mean)\n\n*Compute the loss function based on the Structural Similarity Index Measure (SSIM) Metric.\nFor more info, visit https://vicuesoft.com/glossary/term/ssim-ms-ssim/\nSSIM reference paper: Wang, Zhou, et al. “Image quality assessment: from error visibility to structural similarity.” IEEE transactions on image processing 13.4 (2004): 600-612.*\n\n\nCombined Losses\n\nsource\n\n\nCombinedLoss\n\n CombinedLoss (spatial_dims=2, mse_weight=0.33, mae_weight=0.33)\n\nlosses combined\n\nsource\n\n\nMSSSIMLoss\n\n MSSSIMLoss (spatial_dims=2, window_size:int=8, sigma:float=1.5,\n             reduction:str='mean', levels:int=3, weights=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspatial_dims\nint\n2\nNumber of spatial dimensions.\n\n\nwindow_size\nint\n8\nSize of the Gaussian filter for SSIM.\n\n\nsigma\nfloat\n1.5\nStandard deviation of the Gaussian filter.\n\n\nreduction\nstr\nmean\nSpecifies the reduction to apply to the output (‘mean’, ‘sum’, or ‘none’).\n\n\nlevels\nint\n3\nNumber of scales to use for MS-SSIM.\n\n\nweights\nNoneType\nNone\nWeights to apply to each scale. If None, default values are used.\n\n\n\n\nmsssim_loss = MSSSIMLoss(levels=3)\nssim_loss = SSIMLoss(2)\noutput = torch.rand(10, 3, 64, 64).cuda()  # Example output\ntarget = torch.rand(10, 3, 64, 64).cuda()  # Example target\nloss = msssim_loss(output, target)\nloss2 = ssim_loss(output,target)\nprint(\"ms-ssim: \",loss, '\\nssim: ', loss2)\n\nms-ssim:  tensor(0.9686, device='cuda:0') \nssim:  tensor(0.9949, device='cuda:0')\n\n\n\nsource\n\n\nMSSSIML1Loss\n\n MSSSIML1Loss (spatial_dims=2, alpha:float=0.025, window_size:int=8,\n               sigma:float=1.5, reduction:str='mean', levels:int=3,\n               weights=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspatial_dims\nint\n2\nNumber of spatial dimensions.\n\n\nalpha\nfloat\n0.025\nWeighting factor between MS-SSIM and L1 loss.\n\n\nwindow_size\nint\n8\nSize of the Gaussian filter for SSIM.\n\n\nsigma\nfloat\n1.5\nStandard deviation of the Gaussian filter.\n\n\nreduction\nstr\nmean\nSpecifies the reduction to apply to the output (‘mean’, ‘sum’, or ‘none’).\n\n\nlevels\nint\n3\nNumber of scales to use for MS-SSIM.\n\n\nweights\nNoneType\nNone\nWeights to apply to each scale. If None, default values are used.\n\n\n\n\nmsssiml1_loss = MSSSIML1Loss(alpha=0.025, window_size=11, sigma=1.5, levels=3)\ninput_image = torch.randn(4, 1, 128, 128)  # Batch of 4 grayscale images (1 channel)\ntarget_image = torch.randn(4, 1, 128, 128)\n\n# Compute MSSSIM + Gaussian-weighted L1 loss\nloss = msssiml1_loss(input_image, target_image)\nloss2 = ssim_loss(input_image, target_image)\nprint(\"ms-ssim: \", loss, '\\nssim: ', loss2)\n\nms-ssim:  tensor(0.0250) \nssim:  tensor(0.9955)\n\n\n\nsource\n\n\nMSSSIML2Loss\n\n MSSSIML2Loss (spatial_dims=2, alpha:float=0.1, window_size:int=11,\n               sigma:float=1.5, reduction:str='mean', levels:int=3,\n               weights=None)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nspatial_dims\nint\n2\nNumber of spatial dimensions.\n\n\nalpha\nfloat\n0.1\nWeighting factor between MS-SSIM and L2 loss.\n\n\nwindow_size\nint\n11\nSize of the Gaussian window for SSIM.\n\n\nsigma\nfloat\n1.5\nStandard deviation of the Gaussian.\n\n\nreduction\nstr\nmean\nSpecifies the reduction to apply to the output (‘mean’, ‘sum’, or ‘none’).\n\n\nlevels\nint\n3\nNumber of scales to use for MS-SSIM.\n\n\nweights\nNoneType\nNone\nWeights to apply to each scale. If None, default values are used.\n\n\n\n\nmsssim_l2_loss = MSSSIML2Loss()\noutput = torch.rand(10, 3, 64, 64).cuda()  # Example output with even dimensions\ntarget = torch.rand(10, 3, 64, 64).cuda()  # Example target with even dimensions\nloss = msssim_l2_loss(output, target)\nprint(loss)\n\ntensor(0.0956, device='cuda:0')\n\n\n\n\nCrossEntropy and Dice Loss\n\nsource\n\n\nCrossEntropyLossFlat3D\n\n CrossEntropyLossFlat3D (*args, axis:int=-1, weight=None,\n                         ignore_index=-100, reduction='mean',\n                         flatten:bool=True, floatify:bool=False,\n                         is_2d:bool=True)\n\nSame as nn.CrossEntropyLoss, but flattens input and target for 3D inputs.\n\nsource\n\n\nDiceLoss\n\n DiceLoss (smooth=1)\n\n*DiceLoss computes the Sørensen–Dice coefficient loss, which is often used for evaluating the performance of image segmentation algorithms.\nThe Dice coefficient is a measure of overlap between two samples. It ranges from 0 (no overlap) to 1 (perfect overlap). The Dice loss is computed as 1 - Dice coefficient, so it ranges from 1 (no overlap) to 0 (perfect overlap).\nAttributes: smooth (float): A smoothing factor to avoid division by zero and ensure numerical stability.\nMethods: forward(inputs, targets): Computes the Dice loss between the predicted probabilities (inputs) and the ground truth (targets).*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsmooth\nint\n1\nSmoothing factor to avoid division by zero\n\n\n\n\n# inputs and targets must be equally dimensional tensors\nfrom torch import randn, randint\n\n\ninputs = randn((1, 1, 256, 256))  # Input\ntargets = randint(0, 2, (1, 1, 256, 256)).float()  # Ground Truth\n\n# Initialize\ndice_loss = DiceLoss()\n\n# Compute loss\nloss = dice_loss(inputs, targets)\nprint('Dice Loss:', loss.item())\n\nDice Loss: 0.4982335567474365\n\n\n\n\nFourier Ring Correlation\n\nsource\n\n\nFRCLoss\n\n FRCLoss (image1, image2)\n\n*Compute the Fourier Ring Correlation (FRC) loss between two images.\nReturns: - torch.Tensor: The FRC loss.*\n\n\n\n\nDetails\n\n\n\n\nimage1\nThe first input image.\n\n\nimage2\nThe second input image.\n\n\n\n\nsource\n\n\nFCRCutoff\n\n FCRCutoff (image1, image2)\n\n*Calculate the cutoff frequency at when Fourier ring correlation drops to 1/7.\nReturns: - float: The cutoff frequency.*\n\n\n\n\nDetails\n\n\n\n\nimage1\nThe first input image.\n\n\nimage2\nThe second input image."
  },
  {
    "objectID": "transforms.html",
    "href": "transforms.html",
    "title": "Transforms",
    "section": "",
    "text": "source\n\n\n\n Resample (sampling, **kwargs)\n\n*A subclass of Spacing that handles image resampling based on specified sampling factors or voxel dimensions.\nThe Resample class inherits from Spacing and provides a flexible way to adjust the spacing (voxel size) of images by specifying either a sampling factor or explicitly providing new voxel dimensions.*\n\n\n\n\nDetails\n\n\n\n\nsampling\nSampling factor for isotropic resampling\n\n\nkwargs\n\n\n\n\n\nfrom bioMONAI.core import cells3d, img2Tensor\nfrom bioMONAI.visualize import visualize_slices\n\n\nimg = BioImageStack(img2Tensor(cells3d()[:,0]))\nvisualize_slices(img, showlines=False)\n\nimg2 = Resample(4)(img)\nvisualize_slices(img2, showlines=False)\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n Resize (size=None, **kwargs)\n\n*A subclass of Reshape that handles image resizing based on specified target dimensions.\nThe Resize class inherits from Reshape and provides a flexible way to adjust the size of images by specifying either a target size or scaling factors.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nNoneType\nNone\nTarget dimensions for resizing (height, width). If its length is smaller than the spatial dimensions, values will be repeated. If an int is provided, it will be broadcast to all spatial dimensions.\n\n\nkwargs\n\n\n\n\n\n\n\nprint(img.size())\nvisualize_slices(img, showlines=False)\n\nimg2 = Resize(50)(img)\nprint(img2.size())\nvisualize_slices(img2, showlines=False)\n\ntorch.Size([60, 256, 256])\ntorch.Size([60, 50, 50])"
  },
  {
    "objectID": "transforms.html#size-sampling",
    "href": "transforms.html#size-sampling",
    "title": "Transforms",
    "section": "",
    "text": "source\n\n\n\n Resample (sampling, **kwargs)\n\n*A subclass of Spacing that handles image resampling based on specified sampling factors or voxel dimensions.\nThe Resample class inherits from Spacing and provides a flexible way to adjust the spacing (voxel size) of images by specifying either a sampling factor or explicitly providing new voxel dimensions.*\n\n\n\n\nDetails\n\n\n\n\nsampling\nSampling factor for isotropic resampling\n\n\nkwargs\n\n\n\n\n\nfrom bioMONAI.core import cells3d, img2Tensor\nfrom bioMONAI.visualize import visualize_slices\n\n\nimg = BioImageStack(img2Tensor(cells3d()[:,0]))\nvisualize_slices(img, showlines=False)\n\nimg2 = Resample(4)(img)\nvisualize_slices(img2, showlines=False)\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n Resize (size=None, **kwargs)\n\n*A subclass of Reshape that handles image resizing based on specified target dimensions.\nThe Resize class inherits from Reshape and provides a flexible way to adjust the size of images by specifying either a target size or scaling factors.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nNoneType\nNone\nTarget dimensions for resizing (height, width). If its length is smaller than the spatial dimensions, values will be repeated. If an int is provided, it will be broadcast to all spatial dimensions.\n\n\nkwargs\n\n\n\n\n\n\n\nprint(img.size())\nvisualize_slices(img, showlines=False)\n\nimg2 = Resize(50)(img)\nprint(img2.size())\nvisualize_slices(img2, showlines=False)\n\ntorch.Size([60, 256, 256])\ntorch.Size([60, 50, 50])"
  },
  {
    "objectID": "transforms.html#noise",
    "href": "transforms.html#noise",
    "title": "Transforms",
    "section": "Noise",
    "text": "Noise\n\nsource\n\nRandCameraNoise\n\n RandCameraNoise (p:float=1.0, damp=0.01, qe=0.7, gain=2, offset=100,\n                  exp_time=0.1, dark_current=0.6, readout=1.5,\n                  bitdepth=16, seed=42, simulation=False, camera='cmos',\n                  gain_variance=0.1, offset_variance=5)\n\n*Simulates camera noise by adding Poisson shot noise, dark current noise, and optionally CMOS fixed pattern noise.\nReturns: numpy.ndarray: The noisy image as a NumPy array with dimensions of input_image.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\np\nfloat\n1.0\nProbability of applying Transform\n\n\ndamp\nfloat\n0.01\nDampening factor to prevent saturation when adding noise\n\n\nqe\nfloat\n0.7\nQuantum efficiency of the camera (0 to 1).\n\n\ngain\nint\n2\nCamera gain factor. If an array, it should be broadcastable with input_image shape.\n\n\noffset\nint\n100\nCamera offset in ADU. If an array, it should be broadcastable with input_image shape.\n\n\nexp_time\nfloat\n0.1\nExposure time in seconds.\n\n\ndark_current\nfloat\n0.6\nDark current per pixel in electrons/second.\n\n\nreadout\nfloat\n1.5\nReadout noise standard deviation in electrons.\n\n\nbitdepth\nint\n16\nBit depth of the camera output.\n\n\nseed\nint\n42\nSeed for random number generator for reproducibility.\n\n\nsimulation\nbool\nFalse\nIf True, assumes input_image is already in units of photons and does not convert from electrons.\n\n\ncamera\nstr\ncmos\nSpecifies the type of camera (‘cmos’ or any other). Used to add CMOS fixed pattern noise if ‘cmos’ is specified.\n\n\ngain_variance\nfloat\n0.1\nVariance for the gain noise in CMOS cameras. Only applicable if camera type is ‘cmos’.\n\n\noffset_variance\nint\n5\nVariance for the offset noise in CMOS cameras. Only applicable if camera type is ‘cmos’.\n\n\n\n\nfrom bioMONAI.visualize import plot_image\n\n\nimg3 = img[30]\n\n\n# Original clean image\nplot_image(img3)\n\n\n\n\n\n# Noisy image simulating a CMOS camera\nplot_image(RandCameraNoise(camera = 'cmos').encodes(img3))\n\n\n\n\n\n# Noisy image simulating a CCD camera\nplot_image(RandCameraNoise(camera = 'ccd', readout=2).encodes(img3))"
  },
  {
    "objectID": "transforms.html#normalization",
    "href": "transforms.html#normalization",
    "title": "Transforms",
    "section": "Normalization",
    "text": "Normalization\n\nsource\n\nScaleIntensity\n\n ScaleIntensity (x, min=0.0, max=1.0, axis=None, eps=1e-20, dtype=&lt;class\n                 'numpy.float32'&gt;)\n\nImage normalization.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\n\n\nThe input image to scale.\n\n\nmin\nfloat\n0.0\nThe minimum intensity value.\n\n\nmax\nfloat\n1.0\nThe maximum intensity value.\n\n\naxis\nNoneType\nNone\nThe axis or axes along which to compute the minimum and maximum values.\n\n\neps\nfloat\n1e-20\nA small value to prevent division by zero.\n\n\ndtype\ntype\nfloat32\nThe data type to use for the output image.\n\n\n\n\nsource\n\n\nScaleIntensityPercentiles\n\n ScaleIntensityPercentiles (x, pmin=3, pmax=99.8, axis=None, clip=True,\n                            b_min=0.0, b_max=1.0, eps=1e-20, dtype=&lt;class\n                            'numpy.float32'&gt;)\n\nPercentile-based image normalization.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\n\n\nThe input image to scale.\n\n\npmin\nint\n3\nThe minimum percentile value.\n\n\npmax\nfloat\n99.8\nThe maximum percentile value.\n\n\naxis\nNoneType\nNone\nThe axis or axes along which to compute the minimum and maximum values.\n\n\nclip\nbool\nTrue\nIf True, clips the output values to the specified range.\n\n\nb_min\nfloat\n0.0\nThe minimum intensity value.\n\n\nb_max\nfloat\n1.0\nThe maximum intensity value.\n\n\neps\nfloat\n1e-20\nA small value to prevent division by zero.\n\n\ndtype\ntype\nfloat32\nThe data type to use for the output image.\n\n\n\n\nsource\n\n\nScaleIntensityVariance\n\n ScaleIntensityVariance (target_variance=1.0, ndim=2)\n\nScales the intensity variance of an ND image to a target value.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntarget_variance\nfloat\n1.0\nThe desired variance for the scaled intensities.\n\n\nndim\nint\n2\nNumber of spatial dimensions in the image.\n\n\n\n\n# Example usage with a random tensor of shape (1, 3, 256, 256)\nrand_tensor = BioImageBase(torch.rand(1, 3, 256, 256))\n\ntransform = ScaleIntensityVariance(ndim=4)\n\n# Apply the transform to the tensor\nscaled_tensor = transform(rand_tensor)\n\nprint('Original Tensor Variance:', rand_tensor.var().item())\nprint('Scaled Tensor Variance:', scaled_tensor.var().item())\n\nOriginal Tensor Variance: 0.08352072536945343\nScaled Tensor Variance: 0.9999999403953552"
  },
  {
    "objectID": "transforms.html#data-augmentation",
    "href": "transforms.html#data-augmentation",
    "title": "Transforms",
    "section": "Data Augmentation",
    "text": "Data Augmentation\n\nsource\n\nRandCrop2D\n\n RandCrop2D (size:int|tuple, lazy=False, **kwargs)\n\nRandomly crop an image to size\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint | tuple\n\nSize to crop to, duplicated if one value is specified\n\n\nlazy\nbool\nFalse\na flag to indicate whether this transform should execute lazily or not. Defaults to False\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nRandCropND\n\n RandCropND (size:int|tuple, lazy=False, **kwargs)\n\n*Randomly crops an ND image to a specified size.\nThis transform randomly crops an ND image to a specified size during training and performs a center crop during validation. It supports both 2D and 3D images and videos, assuming the first dimension is the batch dimension.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nsize\nint | tuple\n\nSize to crop to, duplicated if one value is specified\n\n\nlazy\nbool\nFalse\na flag to indicate whether this transform should execute lazily or not. Defaults to False\n\n\nkwargs\n\n\n\n\n\n\n\n# Define a random tensor\norig_size = (65, 65)\nrand_tensor = BioImageBase(torch.rand(8, *orig_size))\n\nfor i in range(100):\n    test_eq((8,64,64),RandCropND((64,64))(rand_tensor).shape)\n\n\nsource\n\n\nRandFlip\n\n RandFlip (prob=0.1, spatial_axis=None, ndim=2, lazy=False, **kwargs)\n\nRandomly flips an ND image over a specified axis.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.1\nProbability of flipping\n\n\nspatial_axis\nNoneType\nNone\nSpatial axes along which to flip over. Default is None. The default axis=None will flip over all of the axes of the input array.\n\n\nndim\nint\n2\nNumber of spatial dimensions in the image\n\n\nlazy\nbool\nFalse\nFlag to indicate whether this transform should execute lazily or not. Defaults to False\n\n\nkwargs\n\n\n\n\n\n\n\n# Define a random tensor\norig_size = (1,4,4)\nrand_tensor = BioImageBase(torch.rand(*orig_size))\n\nprint('orig tensor: ', rand_tensor, '\\n')\n\nfor i in range(3):\n    print(RandFlip(prob=.75, spatial_axis=None)(rand_tensor))\n\norig tensor:  metatensor([[[0.4648, 0.4394, 0.0145, 0.4412],\n         [0.2723, 0.8205, 0.4727, 0.9594],\n         [0.6661, 0.9815, 0.9765, 0.4414],\n         [0.2815, 0.6184, 0.0845, 0.0834]]]) \n\nmetatensor([[[0.2815, 0.6184, 0.0845, 0.0834],\n         [0.6661, 0.9815, 0.9765, 0.4414],\n         [0.2723, 0.8205, 0.4727, 0.9594],\n         [0.4648, 0.4394, 0.0145, 0.4412]]])\nmetatensor([[[0.0834, 0.0845, 0.6184, 0.2815],\n         [0.4414, 0.9765, 0.9815, 0.6661],\n         [0.9594, 0.4727, 0.8205, 0.2723],\n         [0.4412, 0.0145, 0.4394, 0.4648]]])\nmetatensor([[[0.4648, 0.4394, 0.0145, 0.4412],\n         [0.2723, 0.8205, 0.4727, 0.9594],\n         [0.6661, 0.9815, 0.9765, 0.4414],\n         [0.2815, 0.6184, 0.0845, 0.0834]]])\n\n\n\nsource\n\n\nRandRot90\n\n RandRot90 (prob=0.1, max_k=3, spatial_axes=(0, 1), ndim=2, lazy=False,\n            **kwargs)\n\nRandomly rotate an ND image by 90 degrees in the plane specified by axes.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nprob\nfloat\n0.1\nProbability of rotating\n\n\nmax_k\nint\n3\nMax number of times to rotate by 90 degrees\n\n\nspatial_axes\ntuple\n(0, 1)\nSpatial axes along which to rotate. Default: (0, 1), this is the first two axis in spatial dimensions.\n\n\nndim\nint\n2\n\n\n\nlazy\nbool\nFalse\nFlag to indicate whether this transform should execute lazily or not. Defaults to False\n\n\nkwargs\n\n\n\n\n\n\n\n# Define a random tensor\norig_size = (1,4,4)\nrand_tensor = BioImageBase(torch.rand(*orig_size))\n\nprint('orig tensor: ', rand_tensor, '\\n')\n\nfor i in range(3):\n    print(RandRot90(prob=.75)(rand_tensor))\n\norig tensor:  metatensor([[[0.6143, 0.4423, 0.1837, 0.9712],\n         [0.5201, 0.4463, 0.2429, 0.4175],\n         [0.7642, 0.9752, 0.8923, 0.8719],\n         [0.0691, 0.7557, 0.9410, 0.0721]]]) \n\nmetatensor([[[0.9712, 0.4175, 0.8719, 0.0721],\n         [0.1837, 0.2429, 0.8923, 0.9410],\n         [0.4423, 0.4463, 0.9752, 0.7557],\n         [0.6143, 0.5201, 0.7642, 0.0691]]])\nmetatensor([[[0.9712, 0.4175, 0.8719, 0.0721],\n         [0.1837, 0.2429, 0.8923, 0.9410],\n         [0.4423, 0.4463, 0.9752, 0.7557],\n         [0.6143, 0.5201, 0.7642, 0.0691]]])\nmetatensor([[[0.0721, 0.9410, 0.7557, 0.0691],\n         [0.8719, 0.8923, 0.9752, 0.7642],\n         [0.4175, 0.2429, 0.4463, 0.5201],\n         [0.9712, 0.1837, 0.4423, 0.6143]]])"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "The module introduces specialized classes to represent various bioimaging data structures, facilitating seamless integration with machine learning workflows.\n\nsource\n\n\n\n MetaResolver (*args, **kwargs)\n\nThe MetaResolver class addresses metaclass conflicts, ensuring compatibility across different data structures. This is particularly useful when integrating with libraries that have specific metaclass requirements.\n\nsource\n\n\n\n\n BioImageBase (*args, **kwargs)\n\n*Serving as the foundational class for bioimaging data, BioImageBase provides core functionalities for image handling. It ensures that instances of specified types are appropriately cast to this class, maintaining consistency in data representation.\nMetaclass casts x to this class if it is of type cls._bypass_type.*\n\nsource\n\n\n\n\n BioImage (*args, **kwargs)\n\nA subclass of BioImageBase, the BioImage class is tailored for handling both 2D and 3D image objects. It offers methods to load images from various formats and provides access to image properties such as shape and dimensions.\n\na = BioImage.create('data_examples/example_tiff.tiff')\nprint(a.shape)\n\ntorch.Size([1, 96, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageStack (*args, **kwargs)\n\nDesigned for 3D image data, BioImageStack extends BioImageBase to manage volumetric images effectively. It includes functionalities for slicing, visualization, and manipulation of 3D data.\n\na = BioImageStack.create('data_examples/example_tiff.tiff')\nprint(a.shape)\n\ntorch.Size([1, 96, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageProject (*args, **kwargs)\n\nThe BioImageProject class represents a 3D image stack as a 2D image using maximum intensity projection. This is particularly useful for visualizing volumetric data in a 2D format, aiding in quick assessments and presentations.\n\na = BioImageProject.create('data_examples/example_tiff.tiff')\na.shape\n\ntorch.Size([1, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageMulti (*args, **kwargs)\n\nFor multi-channel 2D images, BioImageMulti extends BioImageBase to handle data with multiple channels, such as different fluorescence markers in microscopy images.\n\n# Load a 3D image stack as a multichannel image\na = BioImageMulti.create('data_examples/example_tiff.tiff')\n# Differently from BioImageStack, here the third dimension is encoded as channels.\nprint(a.shape)\n\ntorch.Size([96, 512, 512])"
  },
  {
    "objectID": "data.html#data-types",
    "href": "data.html#data-types",
    "title": "Data",
    "section": "",
    "text": "The module introduces specialized classes to represent various bioimaging data structures, facilitating seamless integration with machine learning workflows.\n\nsource\n\n\n\n MetaResolver (*args, **kwargs)\n\nThe MetaResolver class addresses metaclass conflicts, ensuring compatibility across different data structures. This is particularly useful when integrating with libraries that have specific metaclass requirements.\n\nsource\n\n\n\n\n BioImageBase (*args, **kwargs)\n\n*Serving as the foundational class for bioimaging data, BioImageBase provides core functionalities for image handling. It ensures that instances of specified types are appropriately cast to this class, maintaining consistency in data representation.\nMetaclass casts x to this class if it is of type cls._bypass_type.*\n\nsource\n\n\n\n\n BioImage (*args, **kwargs)\n\nA subclass of BioImageBase, the BioImage class is tailored for handling both 2D and 3D image objects. It offers methods to load images from various formats and provides access to image properties such as shape and dimensions.\n\na = BioImage.create('data_examples/example_tiff.tiff')\nprint(a.shape)\n\ntorch.Size([1, 96, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageStack (*args, **kwargs)\n\nDesigned for 3D image data, BioImageStack extends BioImageBase to manage volumetric images effectively. It includes functionalities for slicing, visualization, and manipulation of 3D data.\n\na = BioImageStack.create('data_examples/example_tiff.tiff')\nprint(a.shape)\n\ntorch.Size([1, 96, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageProject (*args, **kwargs)\n\nThe BioImageProject class represents a 3D image stack as a 2D image using maximum intensity projection. This is particularly useful for visualizing volumetric data in a 2D format, aiding in quick assessments and presentations.\n\na = BioImageProject.create('data_examples/example_tiff.tiff')\na.shape\n\ntorch.Size([1, 512, 512])\n\n\n\nsource\n\n\n\n\n BioImageMulti (*args, **kwargs)\n\nFor multi-channel 2D images, BioImageMulti extends BioImageBase to handle data with multiple channels, such as different fluorescence markers in microscopy images.\n\n# Load a 3D image stack as a multichannel image\na = BioImageMulti.create('data_examples/example_tiff.tiff')\n# Differently from BioImageStack, here the third dimension is encoded as channels.\nprint(a.shape)\n\ntorch.Size([96, 512, 512])"
  },
  {
    "objectID": "data.html#data-conversion",
    "href": "data.html#data-conversion",
    "title": "Data",
    "section": "Data conversion",
    "text": "Data conversion\nTo facilitate seamless integration between tensors and bioimaging data structures, the module provides conversion utilities.\n\nsource\n\nTensor2BioImage\n\n Tensor2BioImage (cls:__main__.BioImageBase=&lt;class\n                  '__main__.BioImageStack'&gt;)\n\nThe Tensor2BioImage transform converts tensors into BioImageBase instances, enabling the application of bioimaging-specific methods to tensor data. This is essential for integrating deep learning models with bioimaging workflows."
  },
  {
    "objectID": "data.html#data-blocks-and-dataloader",
    "href": "data.html#data-blocks-and-dataloader",
    "title": "Data",
    "section": "Data Blocks and Dataloader",
    "text": "Data Blocks and Dataloader\nThe module offers classes to construct data blocks and data loaders, streamlining the preparation of datasets for machine learning models.\n\nsource\n\nBioImageBlock\n\n BioImageBlock (cls:__main__.BioImageBase=&lt;class '__main__.BioImage'&gt;)\n\nA TransformBlock tailored for bioimaging data, BioImageBlock facilitates the creation of data processing pipelines, including transformations and augmentations specific to bioimaging.\n\nsource\n\n\nBioDataBlock\n\n BioDataBlock (blocks:list=(&lt;fastai.data.block.TransformBlock object at\n               0x7f8c50c22410&gt;, &lt;fastai.data.block.TransformBlock object\n               at 0x7f8c3f7276d0&gt;), dl_type:fastai.data.core.TfmdDL=None,\n               get_items=&lt;function get_image_files&gt;, get_y=None,\n               get_x=None, getters:list=None, n_inp:int=None,\n               item_tfms:list=None, batch_tfms:list=None, **kwargs)\n\nThe BioDataBlock class serves as a generic container to build Datasets and DataLoaders efficiently. It integrates item and batch transformations, getters, and splitters, simplifying the setup of data pipelines for training and validation.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nblocks\nlist\n(&lt;fastai.data.block.TransformBlock object at 0x7f8c50c22410&gt;, &lt;fastai.data.block.TransformBlock object at 0x7f8c3f7276d0&gt;)\nOne or more TransformBlocks\n\n\ndl_type\nTfmdDL\nNone\nTask specific TfmdDL, defaults to block’s dl_type orTfmdDL\n\n\nget_items\nfunction\nget_image_files\n\n\n\nget_y\nNoneType\nNone\n\n\n\nget_x\nNoneType\nNone\n\n\n\ngetters\nlist\nNone\nGetter functions applied to results of get_items\n\n\nn_inp\nint\nNone\nNumber of inputs\n\n\nitem_tfms\nlist\nNone\nItemTransforms, applied on an item\n\n\nbatch_tfms\nlist\nNone\nTransforms or RandTransforms, applied by batch\n\n\nkwargs\n\n\n\n\n\n\n\nsource\n\n\nBioDataLoaders\n\n BioDataLoaders (*loaders, path:str|pathlib.Path='.', device=None)\n\nBasic wrapper around several DataLoaders with factory methods for biomedical imaging problems. Managing multiple DataLoader instances, BioDataLoaders handles data loading for different phases of model training, such as training, validation, and testing. It ensures efficient data handling and supports various batch processing strategies.\n\nsource\n\n\nBioDataLoaders.from_source\n\n BioDataLoaders.from_source (data_source, show_summary:bool=False,\n                             path:str|Path='.', bs:int=64,\n                             val_bs:int=None, shuffle:bool=True,\n                             device=None)\n\n*Create and return a DataLoader from a BioDataBlock using provided keyword arguments.\nReturns a DataLoader: A PyTorch DataLoader object populated with the data from the BioDataBlock. If show_summary is True, it also prints a summary of the datablock after creation.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_source\n\n\nThe source of the data to be loaded by the dataloader. This can be any type that is compatible with the dataloading method specified in kwargs (e.g., paths, datasets).\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.from_folder\n\n BioDataLoaders.from_folder (path, get_target_fn, train='train',\n                             valid='valid', valid_pct=None, seed=None,\n                             item_tfms=None, batch_tfms=None,\n                             img_cls=&lt;class '__main__.BioImage'&gt;,\n                             target_img_cls=&lt;class '__main__.BioImage'&gt;,\n                             show_summary:bool=False, bs:int=64,\n                             val_bs:int=None, shuffle:bool=True,\n                             device=None)\n\nCreate from dataset in path with train and valid subfolders (or provide valid_pct)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nget_target_fn\n\n\n\n\n\ntrain\nstr\ntrain\n\n\n\nvalid\nstr\nvalid\n\n\n\nvalid_pct\nNoneType\nNone\n\n\n\nseed\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\ntarget_img_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.from_df\n\n BioDataLoaders.from_df (df, path='.', valid_pct=0.2, seed=None, fn_col=0,\n                         folder=None, pref=None, suff='', target_col=1,\n                         target_folder=None, target_suff='',\n                         valid_col=None, item_tfms=None, batch_tfms=None,\n                         img_cls=&lt;class '__main__.BioImage'&gt;,\n                         target_img_cls=&lt;class '__main__.BioImage'&gt;,\n                         show_summary:bool=False, bs:int=64,\n                         val_bs:int=None, shuffle:bool=True, device=None)\n\nCreate from df using fn_col and target_col\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\npref\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\ntarget_col\nint\n1\n\n\n\ntarget_folder\nNoneType\nNone\n\n\n\ntarget_suff\nstr\n\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\ntarget_img_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.from_csv\n\n BioDataLoaders.from_csv (path, csv_fname='train.csv', header='path',\n                          delimiter=None, quoting=0, valid_pct=0.2,\n                          seed=None, fn_col=0, folder=None, pref=None,\n                          suff='', target_col=1, target_folder=None,\n                          target_suff='', valid_col=None, item_tfms=None,\n                          batch_tfms=None, img_cls=&lt;class\n                          '__main__.BioImage'&gt;, target_img_cls=&lt;class\n                          '__main__.BioImage'&gt;, show_summary:bool=False,\n                          bs:int=64, val_bs:int=None, shuffle:bool=True,\n                          device=None)\n\nCreate from path/csv_fname using fn_col and target_col\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\ncsv_fname\nstr\ntrain.csv\n\n\n\nheader\nstr\npath\n\n\n\ndelimiter\nNoneType\nNone\n\n\n\nquoting\nint\n0\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\npref\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\ntarget_col\nint\n1\n\n\n\ntarget_folder\nNoneType\nNone\n\n\n\ntarget_suff\nstr\n\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\ntarget_img_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.class_from_folder\n\n BioDataLoaders.class_from_folder (path, train='train', valid='valid',\n                                   valid_pct=None, seed=None, vocab=None,\n                                   item_tfms=None, batch_tfms=None,\n                                   img_cls=&lt;class '__main__.BioImage'&gt;,\n                                   show_summary:bool=False, bs:int=64,\n                                   val_bs:int=None, shuffle:bool=True,\n                                   device=None)\n\nCreate from dataset in path with train and valid subfolders (or provide valid_pct)\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\ntrain\nstr\ntrain\n\n\n\nvalid\nstr\nvalid\n\n\n\nvalid_pct\nNoneType\nNone\n\n\n\nseed\nNoneType\nNone\n\n\n\nvocab\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.class_from_df\n\n BioDataLoaders.class_from_df (df, path='.', valid_pct=0.2, seed=None,\n                               fn_col=0, folder=None, suff='',\n                               label_col=1, label_delim=None,\n                               y_block=None, valid_col=None,\n                               item_tfms=None, batch_tfms=None,\n                               img_cls=&lt;class '__main__.BioImage'&gt;,\n                               show_summary:bool=False, bs:int=64,\n                               val_bs:int=None, shuffle:bool=True,\n                               device=None)\n\nCreate from df using fn_col and label_col\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndf\n\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\nlabel_col\nint\n1\n\n\n\nlabel_delim\nNoneType\nNone\n\n\n\ny_block\nNoneType\nNone\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.class_from_csv\n\n BioDataLoaders.class_from_csv (path, csv_fname='labels.csv',\n                                header='infer', delimiter=None, quoting=0,\n                                valid_pct=0.2, seed=None, fn_col=0,\n                                folder=None, suff='', label_col=1,\n                                label_delim=None, y_block=None,\n                                valid_col=None, item_tfms=None,\n                                batch_tfms=None, img_cls=&lt;class\n                                '__main__.BioImage'&gt;,\n                                show_summary:bool=False, bs:int=64,\n                                val_bs:int=None, shuffle:bool=True,\n                                device=None)\n\nCreate from path/csv_fname using fn_col and label_col\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\ncsv_fname\nstr\nlabels.csv\n\n\n\nheader\nstr\ninfer\n\n\n\ndelimiter\nNoneType\nNone\n\n\n\nquoting\nint\n0\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nNoneType\nNone\n\n\n\nfn_col\nint\n0\n\n\n\nfolder\nNoneType\nNone\n\n\n\nsuff\nstr\n\n\n\n\nlabel_col\nint\n1\n\n\n\nlabel_delim\nNoneType\nNone\n\n\n\ny_block\nNoneType\nNone\n\n\n\nvalid_col\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders\n\n\n\n\nsource\n\n\nBioDataLoaders.class_from_lists\n\n BioDataLoaders.class_from_lists (path, fnames, labels, valid_pct=0.2,\n                                  seed:int=None, y_block=None,\n                                  item_tfms=None, batch_tfms=None,\n                                  img_cls=&lt;class '__main__.BioImage'&gt;,\n                                  show_summary:bool=False, bs:int=64,\n                                  val_bs:int=None, shuffle:bool=True,\n                                  device=None)\n\nCreate from list of fnames and labels in path\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr | pathlib.Path\n.\nPath to put in DataLoaders\n\n\nfnames\n\n\n\n\n\nlabels\n\n\n\n\n\nvalid_pct\nfloat\n0.2\n\n\n\nseed\nint\nNone\n\n\n\ny_block\nNoneType\nNone\n\n\n\nitem_tfms\nNoneType\nNone\n\n\n\nbatch_tfms\nNoneType\nNone\n\n\n\nimg_cls\nMetaResolver\nBioImage\n\n\n\nshow_summary\nbool\nFalse\nIf True, print a summary of the BioDataBlock after creation.\n\n\nbs\nint\n64\nSize of batch\n\n\nval_bs\nint\nNone\nSize of batch for validation DataLoader\n\n\nshuffle\nbool\nTrue\nWhether to shuffle data\n\n\ndevice\nNoneType\nNone\nDevice to put DataLoaders"
  },
  {
    "objectID": "data.html#data-getters",
    "href": "data.html#data-getters",
    "title": "Data",
    "section": "Data getters",
    "text": "Data getters\nFunctions to retrieve specific data components are provided, aiding in the organization and preprocessing of datasets.\n\nsource\n\nget_gt\n\n get_gt (path_gt, gt_file_name='avg50.png')\n\n*The get_gt function retrieves ground truth data, essential for supervised learning tasks. It ensures that the correct labels or annotations are associated with each data sample.\nThis function constructs a path to a ground truth file based on the given path_gt and gt_file_name.\nIt uses a lambda function to create a new path by appending gt_file_name to the parent directory of the input file, as specified by path_gt.\nReturns a callable: A function that takes a single argument (a filename) and returns a Path object representing the full path to the ground truth file. When called with a filename, this function constructs the path by combining path_gt or the parent directory of the filename with gt_file_name.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath_gt\n\n\nThe base directory where the ground truth files are stored, or a file path from which to derive the parent directory.\n\n\ngt_file_name\nstr\navg50.png\nThe name of the ground truth file.\n\n\n\nSimilar to get_gt, the get_target function fetches target data for training, validation, or testing, facilitating the preparation of datasets for machine learning models.\n\nsource\n\n\nget_target\n\n get_target (path:str, same_filename=True, target_file_prefix='target',\n             signal_file_prefix='signal', relative_path=False)\n\n*Constructs and returns functions for generating file paths to “target” files based on given input parameters.\nThis function defines two nested helper functions within its scope:\n- `construct_target_filename(file_name)`: Constructs a target file name by inserting the specified prefix into the original file name.\n- `generate_target_path(file_name)`: Generates a path to the target file based on whether `same_filename` is set to True or False.\nThe main function returns the appropriate helper function based on the value of same_filename.\nReturns a callable: A function that takes a file name as input and returns its corresponding target file path based on the specified parameters.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\npath\nstr\n\nThe base directory where the files are located. This should be a string representing an absolute or relative path.\n\n\nsame_filename\nbool\nTrue\nIf True, the target file name will match the original file name; otherwise, it will use the specified prefix.\n\n\ntarget_file_prefix\nstr\ntarget\nThe prefix to insert into the target file name if same_filename is False.\n\n\nsignal_file_prefix\nstr\nsignal\nThe prefix used in the original file names that should be replaced by the target prefix.\n\n\nrelative_path\nbool\nFalse\nIf True, it indicates that the path is relative to the parent folder in the path where the input files are located.\n\n\n\n\nprint(get_target('train_folder/target', same_filename=False)('../signal/signal01.tif'))\nprint(get_target('target', relative_path=True)('../train_folder/signal/image01.tif'))\n\ntrain_folder/target/target01.tif\n../train_folder/target/image01.tif\n\n\n\nprint(get_target('GT', relative_path=True, same_filename=False, target_file_prefix=\"image_clean\", signal_file_prefix=\"image_noisy\")('train_folder/signal/image_noisy.tif'))\n\ntrain_folder/GT/image_clean.tif\n\n\nFor tasks involving denoising or noise analysis, get_noisy_pair retrieves pairs of clean and noisy data, enabling the training of models to learn noise reduction.\n\nsource\n\n\nget_noisy_pair\n\n get_noisy_pair (fn)\n\n*Get another “noisy” version of the input file by selecting a file from the same directory.\nThis function first retrieves all image files in the directory of the input file fn (excluding subdirectories). It then selects one of these files at random, ensuring that it is not the original file itself to avoid creating a trivial “noisy” pair.\nParameters:\nfn (Path or str): The path to the original image file. This should be a Path object but accepts string inputs for convenience.\nReturns:\nPath: A Path object pointing to the selected noisy file.*"
  },
  {
    "objectID": "data.html#data-display",
    "href": "data.html#data-display",
    "title": "Data",
    "section": "Data Display",
    "text": "Data Display\nVisualization functions are included to display batches of data and model results, aiding in qualitative assessments and debugging.\n\nshow_batch\n\nshow_batch (x:BioImageBase, y:BioImageBase, samples,\n            ctxs=None, max_n:int=10, nrows:int=None, ncols:int=None,\n            figsize:tuple=None, **kwargs)\n\nThe show_batch function visualizes a batch of data samples, allowing users to inspect the input data and verify preprocessing steps.\nReturns: List[Context]: A list of contexts after displaying the images and labels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nBioImageBase\n\nThe input image data.\n\n\ny\nBioImageBase\n\nThe target label data.\n\n\nsamples\n\n\nList of sample indices to display.\n\n\nctxs\nNoneType\n\nList of contexts for displaying images. If None, create new ones using get_grid().\n\n\nmax_n\nint\n10\nMaximum number of samples to display.\n\n\nnrows\nint\nNone\nNumber of rows in the grid if ctxs are not provided.\n\n\nncols\nint\nNone\nNumber of columns in the grid if ctxs are not provided.\n\n\nfigsize\ntuple\nNone\nFigure size for the image display.\n\n\nkwargs\n\n\nAdditional keyword arguments.\n\n\n\n\n\nshow_results\n\nshow_results (x: BioImageBase, y: BioImageBase, samples,\n              outs, ctxs=None, max_n=10, figsize=None, **kwargs)\n\nAfter model inference, show_results displays the model’s predictions alongside the ground truth, facilitating the evaluation of model performance.\nReturns:\nList[Context]: A list of contexts after displaying the images and labels.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nx\nBioImageBase\n\nThe input image data.\n\n\ny\nBioImageBase\n\nThe target label data.\n\n\nsamples\n\n\nList of sample indices to display.\n\n\nouts\n\n\nList of output predictions corresponding to the samples.\n\n\nctxs\nNoneType\n\nList of contexts for displaying images. If None, create new ones using get_grid().\n\n\nmax_n\nint\n10\n\n\n\nfigsize\ntuple\nNone\nFigure size for the image display.\n\n\nkwargs\n\n\nAdditional keyword arguments."
  },
  {
    "objectID": "data.html#preprocessing",
    "href": "data.html#preprocessing",
    "title": "Data",
    "section": "Preprocessing",
    "text": "Preprocessing\nThe module provides functions for data preprocessing, including patch extraction and dimensionality reduction, essential for preparing data for machine learning models.\n\nsource\n\nextract_patches\n\n extract_patches (data, patch_size, overlap)\n\n*Extracts n-dimensional patches from the input data.\nReturns: - A list of patches as numpy arrays.*\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ndata\nnumpy array of the input data (n-dimensional).\n\n\npatch_size\ntuple of integers defining the size of the patches in each dimension.\n\n\noverlap\nfloat (between 0 and 1) indicating overlap between patches.\n\n\n\nThe extract_patches function divides images into smaller patches, which is useful for training models on localized regions of interest, especially when dealing with high-resolution images.\n\ndata = np.random.rand(100, 100, 3)  # Example 3D data\npatch_size = (64,64,2)\noverlap = 0.5\npatches = extract_patches(data, patch_size, overlap)\nprint(\"Number of generated patches:\", len(patches))\npatches[0].shape\n\nNumber of generated patches: 8\n\n\n(64, 64, 2)\n\n\n\nsource\n\n\nsave_patches_grid\n\n save_patches_grid (data_folder, gt_folder, output_folder, patch_size,\n                    overlap, threshold=None, squeeze_input=True,\n                    squeeze_patches=False, csv_output=True,\n                    train_test_split_ratio=0.8)\n\nLoads n-dimensional data from data_folder and gt_folder, generates patches, and saves them into individual HDF5 files. Each HDF5 file will have datasets with the structure X/patch_idx and y/patch_idx.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_folder\n\n\nPath to the folder containing data files (n-dimensional data).\n\n\ngt_folder\n\n\nPath to the folder containing ground truth (gt) files (n-dimensional data).\n\n\noutput_folder\n\n\nPath to the folder where the HDF5 files will be saved.\n\n\npatch_size\n\n\ntuple of integers defining the size of the patches.\n\n\noverlap\n\n\nfloat (between 0 and 1) defining the overlap between patches.\n\n\nthreshold\nNoneType\nNone\nIf provided, patches with a mean value below this threshold will be discarded.\n\n\nsqueeze_input\nbool\nTrue\n\n\n\nsqueeze_patches\nbool\nFalse\n\n\n\ncsv_output\nbool\nTrue\nIf True, a CSV file listing all patch paths is created.\n\n\ntrain_test_split_ratio\nfloat\n0.8\nRatio of data to split into train and test CSV files (e.g., 0.8 for 80% train).\n\n\n\nAfter extracting patches, save_patches_grid saves them in a grid format, facilitating visualization and inspection of the patches.\n\ndata_folder = '/home/biagio/Code/Noise2Model/_data/Confocal_BPAE_B/raw/1'\ngt_folder = '/home/biagio/Code/Noise2Model/_data/Confocal_BPAE_B/raw/1'\noutput_folder = './_test'\npatch_size = (64,64)\noverlap = 0\nsave_patches_grid(data_folder, gt_folder, output_folder, patch_size, overlap, squeeze_input=True)\n\nProcessing files: 100%|██████████| 50/50 [00:01&lt;00:00, 37.51it/s]\n\n\nCSV files saved to: ./_test/train_patches.csv and ./_test/test_patches.csv\n\n\n\nfrom bioMONAI.io import hdf5_reader, split_hdf_path\nfrom bioMONAI.visualize import plot_image\n\n\nfile_path = './_test/HV110_P0500510000.h5/X/1'\n\nim , _ = hdf5_reader()(file_path)\nplot_image(im)\n\n\n\n\n\nsource\n\n\nextract_random_patches\n\n extract_random_patches (data, patch_size, num_patches)\n\n*Extracts a specified number of random n-dimensional patches from the input data.\nReturns: - A list of randomly cropped patches as numpy arrays.*\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\ndata\nnumpy array of the input data (n-dimensional).\n\n\npatch_size\ntuple of integers defining the size of the patches in each dimension.\n\n\nnum_patches\nnumber of random patches to extract.\n\n\n\n\nsource\n\n\nsave_patches_random\n\n save_patches_random (data_folder, gt_folder, output_folder, patch_size,\n                      num_patches, threshold=None, squeeze_input=True,\n                      squeeze_patches=False, csv_output=True,\n                      train_test_split_ratio=0.8)\n\nLoads n-dimensional data from data_folder and gt_folder, generates random patches, and saves them into individual HDF5 files. Each HDF5 file will have datasets with the structure X/patch_idx and y/patch_idx.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata_folder\n\n\nPath to the folder containing data files (n-dimensional data).\n\n\ngt_folder\n\n\nPath to the folder containing ground truth (gt) files (n-dimensional data).\n\n\noutput_folder\n\n\nPath to the folder where the HDF5 files will be saved.\n\n\npatch_size\n\n\ntuple of integers defining the size of the patches.\n\n\nnum_patches\n\n\nnumber of random patches to extract per file.\n\n\nthreshold\nNoneType\nNone\nIf provided, patches with a mean value below this threshold will be discarded.\n\n\nsqueeze_input\nbool\nTrue\nIf True, squeezes singleton dimensions in the input data.\n\n\nsqueeze_patches\nbool\nFalse\nIf True, squeezes singleton dimensions in the patches.\n\n\ncsv_output\nbool\nTrue\nIf True, a CSV file listing all patch paths is created.\n\n\ntrain_test_split_ratio\nfloat\n0.8\nRatio of data to split into train and test CSV files (e.g., 0.8 for 80% train).\n\n\n\n\ndata_folder = '/home/biagio/Code/Noise2Model/_data/Confocal_BPAE_B/raw/1'\ngt_folder = '/home/biagio/Code/Noise2Model/_data/Confocal_BPAE_B/raw/1'\noutput_folder = './_test2'\npatch_size = (64,64)\nnum_patches= 2\nsave_patches_random(data_folder, gt_folder, output_folder, patch_size, num_patches)\n\nProcessing files: 100%|██████████| 50/50 [00:00&lt;00:00, 57.88it/s]\n\n\nCSV files saved to: ./_test2/train_patches.csv and ./_test2/test_patches.csv\n\n\n\nfile_path = './_test2/HV110_P0500510000_random_patches.h5/X/1'\n\nim , _ = hdf5_reader()(file_path)\nplot_image(im)\n\n\n\n\n\nsource\n\n\ndict2string\n\n dict2string (d, item_sep='_', key_value_sep='', pad_zeroes=None)\n\n*Transforms a dictionary into a string with customizable separators and optional zero padding for integers.\nReturns the formatted dictionary as a string.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nd\n\n\nThe dictionary to convert.\n\n\nitem_sep\nstr\n_\nThe separator between dictionary items (default is “,”).\n\n\nkey_value_sep\nstr\n\nThe separator between keys and values (default is “:”).\n\n\npad_zeroes\nNoneType\nNone\nThe minimum width for integer values, padded with zeros. If None, no padding is applied.\n\n\n\n\nmy_dict = {'C': 2, 'Z': 30, 'S': 1}\nresult = dict2string(my_dict, pad_zeroes=3)\nprint(result)\n\nC002_Z030_S001\n\n\n\nsource\n\n\nremove_singleton_dims\n\n remove_singleton_dims (substack, order)\n\n*Remove dimensions with a size of 1 from both the substack and the order string.\nReturns:\nsubstack (np.array): The substack with singleton dimensions removed.\nnew_order (str): The updated dimension order string.*\n\n\n\n\nDetails\n\n\n\n\nsubstack\nThe extracted substack data.\n\n\norder\nThe dimension order string (e.g., ‘CZYX’).\n\n\n\n\nsource\n\n\nextract_substacks\n\n extract_substacks (input_file, output_dir=None, indices=None,\n                    split_dimension=None, squeeze_dims=True, *kwargs)\n\nExtract substacks from a multidimensional OME-TIFF stack using AICSImageIO.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninput_file\n\n\nPath to the input OME-TIFF file.\n\n\noutput_dir\nNoneType\nNone\nDirectory to save the extracted substacks. If a list, the substacks will be saved in the corresponding subdirectories from the list.\n\n\nindices\nNoneType\nNone\nA dictionary specifying which indices to extract. Keys can include ‘C’ for channel, ‘Z’ for z-slice, ‘T’ for time point, and ‘S’ for scene. If None, all indices are extracted.\n\n\nsplit_dimension\nNoneType\nNone\nDimension to split substacks along. If provided, separate substacks will be generated for each index in the split_dimension. Must be one of the keys in indices.\n\n\nsqueeze_dims\nbool\nTrue\nDimension to squeeze substacks along.\n\n\nkwargs\n\n\n\n\n\n\n\noutput_dir = \"./_test_folder/\"\nsubdirs = [output_dir + folder for folder in [\"channel_0\", \"channel_1\", \"channel_2\"]]\nsubdirs\n\n['./_test_folder/channel_0',\n './_test_folder/channel_1',\n './_test_folder/channel_2']\n\n\n\n[os.path.join([output_dir][0], f\"{subdirs[0]}_{ii}\") for ii in range(2)]\n\n['./_test_folder/./_test_folder/channel_0_0',\n './_test_folder/./_test_folder/channel_0_1']\n\n\n\nfilename = '../_data/aics/2cad3afd_3500001004_100X_20170623_5-Scene-6-P29-F05.ome.tiff'\n\n# This extracts a single substack for channel 0, z-slice 5, and time point 0.\nextract_substacks(filename, output_dir=output_dir, indices={\"C\": 0, \"Z\": range(35), \"T\": 0})\n\nExtracted substack saved to: ./_test_folder/2cad3afd_3500001004_100X_20170623_5-Scene-6-P29-F05_substack_C0_Zrange(0, 35)_T0.ome.tiff\n\n\n\n# This extracts substacks for each channel (`C`) and saves them in separate subfolders named \"C_0\", \"C_1\", \"C_2\", etc.\nextract_substacks(filename, output_dir=[output_dir], indices={\"C\": [0, 1, 2], \"Z\": 5, \"T\": 0}, split_dimension=\"C\")\n\nExtracted substack saved to: ./_test_folder/C_0/2cad3afd_3500001004_100X_20170623_5-Scene-6-P29-F05_substack_C0_Z5_T0.ome.tiff\nExtracted substack saved to: ./_test_folder/C_1/2cad3afd_3500001004_100X_20170623_5-Scene-6-P29-F05_substack_C1_Z5_T0.ome.tiff\nExtracted substack saved to: ./_test_folder/C_2/2cad3afd_3500001004_100X_20170623_5-Scene-6-P29-F05_substack_C2_Z5_T0.ome.tiff\n\n\n\n# This extracts substacks for each channel and saves them in directories \"channel_0\", \"channel_1\", and \"channel_2\".\nextract_substacks(filename, output_dir=subdirs, indices={\"C\": [0, 1, 2], \"Z\": 5}, split_dimension=\"C\")\n\nExtracted substack saved to: ./_test_folder/channel_0/2cad3afd_3500001004_100X_20170623_5-Scene-6-P29-F05_substack_C0_Z5.ome.tiff\nExtracted substack saved to: ./_test_folder/channel_1/2cad3afd_3500001004_100X_20170623_5-Scene-6-P29-F05_substack_C1_Z5.ome.tiff\nExtracted substack saved to: ./_test_folder/channel_2/2cad3afd_3500001004_100X_20170623_5-Scene-6-P29-F05_substack_C2_Z5.ome.tiff"
  },
  {
    "objectID": "Tutorials/tutorial_classification.html",
    "href": "Tutorials/tutorial_classification.html",
    "title": "Image Classification 2D",
    "section": "",
    "text": "Setup imports\n\nfrom bioMONAI.data import *\nfrom bioMONAI.transforms import *\nfrom bioMONAI.core import *\nfrom bioMONAI.core import Path\nfrom bioMONAI.data import get_image_files\nfrom bioMONAI.losses import *\nfrom bioMONAI.metrics import *\nfrom bioMONAI.datasets import download_medmnist\n\nfrom fastai.vision.all import CategoryBlock, GrandparentSplitter, parent_label, resnet34, CrossEntropyLossFlat, accuracy\n\n\ndevice = get_device()\nprint(device)\n\ncuda\n\n\n\n\nDownload dataset\n\nWe’ll employ the publicly available BloodMNIST dataset. The BloodMNIST is based on a dataset of individual normal cells, captured from individuals without infection, hematologic or oncologic disease and free of any pharmacologic treatment at the moment of blood collection. It contains a total of 17,092 images and is organized into 8 classes.\n\n\nbioMONAI includes a dedicated functionality for downloading MedMNIST datasets, accessible via the download_medmnist function.\n\n\nimage_path = '../_data/medmnist_data/'\ninfo = download_medmnist('bloodmnist', image_path, download_only=True)\n\nDataset 'bloodmnist' is already downloaded and available in '../_data/medmnist_data/bloodmnist'.\n\n\n\n\nCreate Dataloader\n\nThe next step is to define the data loading strategy for model training and validation. To this end, we’ll create a DataLoader using the method BioDataLoaders.from_source(), configured with the arguments specified in data_ops.\n\n\nbatch_size = 32\n\npath = Path(image_path)/'bloodmnist'\npath_train = path/'train'\npath_val = path/'val'\n\ndata_ops = {\n    'blocks':       (BioImageBlock(cls=BioImageMulti), CategoryBlock(info['label'])),\n    'get_items':    get_image_files,\n    'get_y':        parent_label,\n    'splitter':     GrandparentSplitter(train_name='train', valid_name='val'),\n    'item_tfms':    [ScaleIntensity(min=0.0, max=1.0), RandRot90(prob=0.5), RandFlip(prob=0.5)],\n    'bs':           batch_size,\n}\n\ndata = BioDataLoaders.from_source(\n    path, \n    show_summary=False,\n    **data_ops,\n    )\n\n# print length of training and validation datasets\nprint('train images:', len(data.train_ds.items), '\\nvalidation images:', len(data.valid_ds.items))\n\ntrain images: 11959 \nvalidation images: 1712\n\n\n\n\nData Visualization\n\nThe show_batch function enables the display of input images and their corresponding targets.\n\n\ndata.show_batch(max_n=4)\n\n\n\n\n\n\nLoad and train the model\n\nmodel = resnet34\n\nloss = CrossEntropyLossFlat()\nmetrics = accuracy\n\ntrainer = visionTrainer(data, model, loss_fn=loss, metrics=metrics, show_summary=False)\n\n\ntrainer.fine_tune(20, freeze_epochs=2)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.708640\n0.683854\n0.777453\n00:10\n\n\n1\n0.576192\n0.405103\n0.858061\n00:10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.299481\n0.193486\n0.936332\n00:11\n\n\n1\n0.293008\n0.228729\n0.918224\n00:10\n\n\n2\n0.272829\n0.214455\n0.926402\n00:09\n\n\n3\n0.305058\n0.591883\n0.805491\n00:10\n\n\n4\n0.300107\n0.269342\n0.910631\n00:09\n\n\n5\n0.284960\n0.268985\n0.907710\n00:10\n\n\n6\n0.324666\n0.301217\n0.892523\n00:10\n\n\n7\n0.464664\n0.377951\n0.877921\n00:10\n\n\n8\n0.248325\n0.231458\n0.924650\n00:09\n\n\n9\n0.221637\n0.188274\n0.936332\n00:10\n\n\n10\n0.202730\n0.250833\n0.921729\n00:10\n\n\n11\n0.167961\n0.246925\n0.910631\n00:10\n\n\n12\n0.147837\n0.146140\n0.944509\n00:10\n\n\n13\n0.164445\n0.146480\n0.943341\n00:09\n\n\n14\n0.126141\n0.103653\n0.963785\n00:10\n\n\n15\n0.112292\n0.123060\n0.960864\n00:10\n\n\n16\n0.091364\n0.090764\n0.966121\n00:10\n\n\n17\n0.096191\n0.093275\n0.964953\n00:09\n\n\n18\n0.073260\n0.085704\n0.966706\n00:10\n\n\n19\n0.080190\n0.087858\n0.966706\n00:10\n\n\n\n\n\n\n\n\n\n\nModel evaluation\n\nevaluate_classification_model(trainer, metrics=metrics, most_confused_n=5, show_graph=False);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.93      0.99      0.96       122\n           1       1.00      1.00      1.00       312\n           2       0.97      0.97      0.97       155\n           3       0.94      0.89      0.91       290\n           4       0.97      0.97      0.97       122\n           5       0.91      0.96      0.93       143\n           6       0.98      0.97      0.97       333\n           7       1.00      1.00      1.00       235\n\n    accuracy                           0.97      1712\n   macro avg       0.96      0.97      0.96      1712\nweighted avg       0.97      0.97      0.97      1712\n\n\nMost Confused Classes:\n[('3', '5', 12), ('6', '3', 9), ('3', '0', 8), ('3', '6', 6), ('5', '3', 6)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCrossEntropyLossFlat\n\n\n\n\n\nMean\n1.313160\n\n\nMedian\n1.274517\n\n\nStandard Deviation\n0.144572\n\n\nMin\n1.274009\n\n\nMax\n2.273876\n\n\nQ1\n1.274059\n\n\nQ3\n1.279382\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\naccuracy\n\n\n\n\n\nMean\n0.966706\n\n\nMedian\n1.000000\n\n\nStandard Deviation\n0.179404\n\n\nMin\n0.000000\n\n\nMax\n1.000000\n\n\nQ1\n1.000000\n\n\nQ3\n1.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSave the model\n\n# trainer.save('tmp-model')\n\n\n\nTest data\nEvaluate the performance of the selected model on unseen data. It’s important to not touch this data until you have fine tuned your model to get an unbiased evaluation!\n\npath_test = path/'test'\n\ntest_data = data.test_dl(get_image_files(path_test).shuffle(), with_labels=True)\n# print length of test dataset\nprint('test images:', len(test_data))\n\ntest images: 107\n\n\n\nevaluate_classification_model(trainer, test_data, metrics=metrics, show_graph=False);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n           0       0.96      0.94      0.95       244\n           1       1.00      1.00      1.00       624\n           2       0.98      0.97      0.98       311\n           3       0.91      0.93      0.92       579\n           4       0.95      0.95      0.95       243\n           5       0.90      0.92      0.91       284\n           6       0.98      0.97      0.97       666\n           7       1.00      1.00      1.00       470\n\n    accuracy                           0.96      3421\n   macro avg       0.96      0.96      0.96      3421\nweighted avg       0.96      0.96      0.96      3421\n\n\nMost Confused Classes:\n[('3', '5', 22), ('6', '3', 20), ('5', '3', 15), ('3', '6', 10), ('0', '3', 9), ('5', '4', 7), ('2', '3', 6), ('3', '0', 5), ('4', '3', 5), ('0', '5', 4), ('3', '4', 4), ('4', '5', 4), ('4', '2', 3), ('5', '0', 2), ('0', '4', 1), ('0', '6', 1), ('1', '0', 1), ('2', '1', 1), ('2', '4', 1), ('2', '6', 1), ('3', '2', 1), ('4', '0', 1), ('6', '1', 1), ('6', '2', 1)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCrossEntropyLossFlat\n\n\n\n\n\nMean\n1.319366\n\n\nMedian\n1.274616\n\n\nStandard Deviation\n0.156896\n\n\nMin\n1.274009\n\n\nMax\n2.274005\n\n\nQ1\n1.274069\n\n\nQ3\n1.280444\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\naccuracy\n\n\n\n\n\nMean\n0.963169\n\n\nMedian\n1.000000\n\n\nStandard Deviation\n0.188348\n\n\nMin\n0.000000\n\n\nMax\n1.000000\n\n\nQ1\n1.000000\n\n\nQ3\n1.000000"
  },
  {
    "objectID": "Tutorials/tutorial_multispectral_classification.html",
    "href": "Tutorials/tutorial_multispectral_classification.html",
    "title": "Multispectral Classification",
    "section": "",
    "text": "from bioMONAI.data import *\nfrom bioMONAI.transforms import *\nfrom bioMONAI.core import *\nfrom bioMONAI.core import Path\nfrom bioMONAI.data import *\nfrom bioMONAI.nets import BasicUNet, DynUNet\nfrom bioMONAI.losses import *\nfrom bioMONAI.losses import SSIMLoss\nfrom bioMONAI.metrics import *\nfrom bioMONAI.datasets import download_file, split_dataframe, add_columns_to_csv\n\nimport os\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndevice = get_device()\nprint(device)\n\ncuda\n\n\n\n\n# Define the base URL for the dataset\nurl = \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/rxrx1_subset_monai.zip\"\n\ndownload_file(url, \"../_data\", extract=True, hash='e80db433db641bb390ade991b81f98814a26c7de30e0da6f20e8abddf7a84538', extract_dir='')\n\nThe file has been downloaded and saved to: ../_data\nExtracted files have been saved to: ../_data\n\n\n\ndata_folder = '../_data/rxrx1_subset_monai/'\ncsv_file = data_folder + 'metadata.csv'\n\nimport pandas as pd\ndf = pd.read_csv(csv_file)\n\nch1, ch2, ch3, ch4, ch5,ch6 = [],[],[],[],[],[]\nfor sid in df['site_id']: \n    site_id = sid.split('_')\n    base_image_path = os.path.join('images', site_id[0], f'Plate{site_id[1]}', f'{site_id[2]}_s{site_id[3]}_w')\n    channel_list = [f'{base_image_path}{i}.png' for i in range(1,7)]\n    ch1.append(channel_list[0])\n    ch2.append(channel_list[1])\n    ch3.append(channel_list[2])\n    ch4.append(channel_list[3])\n    ch5.append(channel_list[4])\n    ch6.append(channel_list[5])\nimage_paths = {'channel 1': ch1, 'channel 2': ch2, 'channel 3': ch3, 'channel 4': ch4, 'channel 5': ch5, 'channel 6': ch6}\n# Let's create a new csv file to avoid overwriting the original one\nnew_csv_file = data_folder + 'metadata_updated.csv'\nadd_columns_to_csv(csv_file, image_paths, new_csv_file)\n\nColumns ['channel 1', 'channel 2', 'channel 3', 'channel 4', 'channel 5', 'channel 6'] added successfully. Updated file saved to '../_data/rxrx1_subset_monai/metadata_updated.csv'\n\n\n\nsplit_dataframe(new_csv_file, \n                train_fraction=0.7, \n                valid_fraction=0.05, \n                split_column='dataset', \n                add_is_valid=True, \n                train_path=\"train.csv\", \n                test_path=\"test.csv\", \n                valid_path=\"valid.csv\", \n                data_save_path=data_folder\n                )\n\nTrain and test files saved as '../_data/rxrx1_subset_monai/train.csv' and '../_data/rxrx1_subset_monai/test.csv' respectively.\n'is_valid' column added to '../_data/rxrx1_subset_monai/train.csv' for validation samples.\n\n\n\nfrom fastai.vision.all import RandomResizedCrop\n\nbs = 8\n\nitemTfms = [ScaleIntensityPercentiles(1,99), RandomResizedCrop(512,min_scale=0.9, max_scale=1.1), RandRot90(prob=.75), RandFlip(prob=0.5)]\nbatchTfms = []\n\ndata = BioDataLoaders.class_from_csv(\n    data_folder,\n    'train.csv',\n    fn_col=[12,13,14,15,16,17],\n    label_col=3,\n    valid_col=-1,\n    seed=42, \n    img_cls=BioImageMulti,\n    item_tfms=itemTfms,\n    batch_tfms=batchTfms, \n    show_summary=False,\n    bs = bs,\n    )\n\n# print length of training and validation datasets\nprint('train images:', len(data.train_ds.items), '\\nvalidation images:', len(data.valid_ds.items))\n\ntrain images: 929 \nvalidation images: 71\n\n\n\ndata.show_batch(max_slices=6, layout='multirow')\n\n\n\n\n\na = data.do_item(100)\na[0].show(max_slices=6, layout='multirow');\n\n\n\n\n\n\nfrom monai.networks.nets import DenseNet169\n\nmodel = DenseNet169(spatial_dims=2, in_channels=6, out_channels=data.c, pretrained=True)\n\n\nfrom fastai.vision.all import RocAuc, accuracy\nmetrics = [RocAuc(), accuracy]\n\ntrainer = fastTrainer(data, model, metrics=metrics, show_summary=False)\n\n\ntrainer.fine_tune(4, freeze_epochs=2)\n\n\n\n\n\n\n    \n      \n      0.00% [0/2 00:00&lt;?]\n    \n    \n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nroc_auc_score\naccuracy\ntime\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/116 00:00&lt;?]\n    \n    \n\n\n\ntrainer.save('multispectral-classification-model')\n\nPath('../_data/rxrx1_subset_monai/models/multispectral-classification-model.pth')\n\n\n\n\ntest_data = BioDataLoaders.class_from_csv(\n    data_folder,\n    'test.csv',\n    fn_col=[12,13,14,15,16,17],\n    label_col=3,\n    valid_pct=0,\n    seed=42, \n    img_cls=BioImageMulti,\n    item_tfms=[ScaleIntensityPercentiles(1,99)],\n    batch_tfms=batchTfms, \n    show_summary=False,\n    bs = 50,\n    )\n\n\nscores = evaluate_classification_model(trainer, test_data, metrics=accuracy, show_graph=False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n              precision    recall  f1-score   support\n\n       HEPG2       0.84      0.86      0.85        50\n       HUVEC       0.86      1.00      0.93        50\n         RPE       0.64      0.84      0.72        50\n        U2OS       1.00      0.50      0.67        50\n\n    accuracy                           0.80       200\n   macro avg       0.84      0.80      0.79       200\nweighted avg       0.84      0.80      0.79       200\n\n\nMost Confused Classes:\n[('U2OS', 'RPE', 17), ('RPE', 'HUVEC', 8), ('U2OS', 'HEPG2', 8), ('HEPG2', 'RPE', 7)]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\nCrossEntropyLossFlat\n\n\n\n\n\nMean\n0.961880\n\n\nMedian\n0.769793\n\n\nStandard Deviation\n0.330322\n\n\nMin\n0.743672\n\n\nMax\n1.741814\n\n\nQ1\n0.745767\n\n\nQ3\n1.020655\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\n\n\naccuracy\n\n\n\n\n\nMean\n0.8\n\n\nMedian\n1.0\n\n\nStandard Deviation\n0.4\n\n\nMin\n0.0\n\n\nMax\n1.0\n\n\nQ1\n1.0\n\n\nQ3\n1.0"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "bioMONAI",
    "section": "Overview",
    "text": "Overview\nbioMONAI is a low-code Python-based platform for developing and deploying deep learning models in biomedical imaging built on top of the MONAI framework, fastai, and TorchIO. This project aims to facilitate interoperability, reproducibility, and community collaboration in biomedical research.\n\nFor more information, bioMONAI documentation can be found here."
  },
  {
    "objectID": "index.html#table-of-contents",
    "href": "index.html#table-of-contents",
    "title": "bioMONAI",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nbioMONAI\n\nOverview\nTable of Contents\nInstallation\nGetting Started\nUsage\nContributing\nLicense\nContact"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "bioMONAI",
    "section": "Installation",
    "text": "Installation\nTo install the bioMONAI environment, follow these steps:\n\nClone the repository:\ngit clone https://github.com/deepclem/biomonai.git\ncd biomonai\nCreate a new Conda environment and install dependencies:\nconda env create --file bioMONAI-env.yml\nActivate the environment and install MONAI:\nconda activate bioMONAI-env\npip install -e ."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "bioMONAI",
    "section": "Getting Started",
    "text": "Getting Started\nTo get started with bioMONAI, we recommend exploring our tutorials, which guide you through model training for various tasks such as classification and denoising.\n\n\n\nNotebook\nOpen in Colab\n\n\n\n\nTutorial: Classification 2D  This notebook provides a comprehensive guide on training deep learning models for 2D image classification tasks, covering data loading, preprocessing, model building, training, and evaluation.\n\n\n\nTutorial: Denoising 2D  This notebook offers a detailed guide on applying deep learning techniques to denoise biological microscopy images. It covers data preparation, model architecture, training processes, and evaluation methods, providing a comprehensive resource for enhancing image quality in biological research."
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "bioMONAI",
    "section": "Usage",
    "text": "Usage\nTo use bioMONAI for your own projects, follow these steps:\n\nCreate a new Jupyter notebook or open an existing one.\nImport necessary modules:\nimport bioMONAI\nStart coding! You can now leverage MONAI’s capabilities alongside the interactive features of Jupyter notebooks."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "bioMONAI",
    "section": "Contributing",
    "text": "Contributing\nWe welcome contributions from the community! To contribute to BioMONAI nbs, follow these steps:\n\nFork the repository on GitHub.\nClone your fork:\ngit clone https://github.com/your_username/biomonai.git\ncd biomonai\nCreate a new branch for your changes:\ngit checkout -b feature/new-feature\nMake your changes and commit them:\ngit add .\ngit commit -m \"Add new feature: &lt;feature description&gt;\"\nPush to your fork and create a pull request on GitHub.\nWait for the review, and merge if everything looks good!"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "bioMONAI",
    "section": "License",
    "text": "License\nbioMONAI is released under the Apache 2.0 license. See LICENSE for more details."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "bioMONAI",
    "section": "Contact",
    "text": "Contact\nIf you have any questions or need further assistance, please open an issue on GitHub or contact us directly at:\n\nProject Lead: Biagio Mandracchia\nContributors: Sara Cruz-Adrados, Juan Pita-López, Rosa-María Menchón-Lara"
  },
  {
    "objectID": "io.html#image-writers",
    "href": "io.html#image-writers",
    "title": "I/O",
    "section": "Image Writers",
    "text": "Image Writers\n\nsource\n\nwrite_image\n\n write_image (data, file_path, dimension_order='TCZYX')\n\n*Writes an image to a file.\n:param data: Image data (numpy array, tensor, or AICSImage) :param file_path: Path to save the image :param format: Format to save the image in (default is png)*\n\n# Example usage:\nnumpy_array = np.random.rand(3, 100, 100)\nwrite_image(numpy_array, './data_examples/output_from_numpy.tiff')\n\ntensor = torch.rand(3, 100, 100)\nwrite_image(tensor, './data_examples/output_from_tensor.tiff')\n\naics_image = AICSImage('./data_examples/example_tiff.tiff')\nwrite_image(aics_image, './data_examples/output_from_tiff.png')\n\nImage successfully saved to ./data_examples/output_from_numpy.tiff\nImage successfully saved to ./data_examples/output_from_tensor.tiff\nImage successfully saved to ./data_examples/output_from_tiff.png"
  },
  {
    "objectID": "io.html#image-readers",
    "href": "io.html#image-readers",
    "title": "I/O",
    "section": "Image Readers",
    "text": "Image Readers\n\nsource\n\ntiff2torch\n\n tiff2torch (file_path:str)\n\nLoad tiff into pytorch tensor\n\nsource\n\n\nstring2dict\n\n string2dict (input_string:str)\n\n\nsource\n\n\nsplit_path\n\n split_path (file_path,\n             exts:(&lt;class'fastcore.foundation.L'&gt;,&lt;class'list'&gt;)=['.ome.ti\n             ff', '.tiff', '.tif', '.png'])\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\nThe path to the file to split\n\n\nexts\n(&lt;class ‘fastcore.foundation.L’&gt;, &lt;class ‘list’&gt;)\n[‘.ome.tiff’, ‘.tiff’, ‘.tif’, ‘.png’]\nList of filename extensions\n\n\n\n\nsource\n\n\naics_image_reader\n\n aics_image_reader (ind_dict=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nind_dict\nNoneType\nNone\nDictionary indicating the channels to load\n\n\n\n\nfile_path = 'data_examples/example_tiff.tiff'\ntest_img, _ = aics_image_reader({'Z': 0})(file_path)\ntest_img.shape\n\n(1, 1, 512, 512)\n\n\n\n\nHierarchical Data Format\n\nsource\n\n\nsplit_hdf_path\n\n split_hdf_path (file_path,\n                 hdf5_exts:(&lt;class'fastcore.foundation.L'&gt;,&lt;class'list'&gt;)=\n                 ['.h5', '.hdf5'])\n\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n\n\nThe path to the HDF5 file to split\n\n\nhdf5_exts\n(&lt;class ‘fastcore.foundation.L’&gt;, &lt;class ‘list’&gt;)\n[‘.h5’, ‘.hdf5’]\nList of filename extensions\n\n\n\n\nsource\n\n\nhdf5_reader\n\n hdf5_reader (dataset=None, patch=0,\n              hdf5_exts:(&lt;class'fastcore.foundation.L'&gt;,&lt;class'list'&gt;)=['.\n              h5', '.hdf5'])\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\nNoneType\nNone\nThe dataset to load\n\n\npatch\nint\n0\nThe patch to load from the dataset\n\n\nhdf5_exts\n(&lt;class ‘fastcore.foundation.L’&gt;, &lt;class ‘list’&gt;)\n[‘.h5’, ‘.hdf5’]\nList of filename extensions\n\n\n\nImages can be loaded by explicitly writing dataset name and path number…\n\nfrom bioMONAI.visualize import plot_image\n\n\nfile_path = './data_examples/0450_1.hdf5'\ndataset_name='clean'\npatch_num=10\n\nim , _ = hdf5_reader(dataset=dataset_name, patch=patch_num)(file_path)\nplot_image(im[0])\n\n\n\n\n… or enconding them in the path, where datasets are subfolders and patches the image files. The latter being compatible with image_reader syntaxis.\n\nf = file_path + '/' + dataset_name + '/' + '%d'%(patch_num)\nim , _ = hdf5_reader()(f)\nplot_image(im[0])\n\n\n\n\n\n\nPreprocessing\n\n\nLoad and preprocess\n\norg_img, _, _ = _load_and_preprocess(f)\n\ntest_eq(org_img.data[0].shape, im.shape)\n\n\n\nRead multichannel data\n\nt = _multi_sequence([f], only_tensor=True);\ntest_eq(t[0].shape, im.shape)\n\n\nt.shape\n\ntorch.Size([1, 1, 96, 96])\n\n\n\n\nImage reader\n\nsource\n\n\nimage_reader\n\n image_reader (file_path:(&lt;class'str'&gt;,&lt;class'pathlib.Path'&gt;,&lt;class'fastco\n               re.foundation.L'&gt;,&lt;class'list'&gt;), dtype=&lt;class\n               'torch.Tensor'&gt;, only_tensor:bool=True, **kwargs)\n\n*Loads and preprocesses a medical image.\nArgs: file_path: Path to the image. Can be a string, Path object or a list. dtype: Datatype for the return value. Defaults to torchTensor. reorder: Whether to reorder the data to be closest to canonical (RAS+) orientation. Defaults to False. resample: Whether to resample image to different voxel sizes and image dimensions. Defaults to None. only_tensor: To return only an image tensor. Defaults to True.\nReturns: The preprocessed image. Returns only the image tensor if only_tensor is True, otherwise returns original image, preprocessed image, and original size.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nfile_path\n(&lt;class ‘str’&gt;, &lt;class ‘pathlib.Path’&gt;, &lt;class ‘fastcore.foundation.L’&gt;, &lt;class ‘list’&gt;)\n\nPath to the image\n\n\ndtype\n_TensorMeta\nTensor\nDatatype for the return value. Defaults to torchTensor\n\n\nonly_tensor\nbool\nTrue\nTo return only an image tensor\n\n\nkwargs\n\n\n\n\n\n\n\ntest_eq(image_reader(f)[0].shape, im.shape)"
  },
  {
    "objectID": "callbacks.html",
    "href": "callbacks.html",
    "title": "Callbacks",
    "section": "",
    "text": "Callbacks that add functionlities during the training phase, including Callbacks that make decisions depending how a monitored metric/loss behaves\n\nsource\n\n\n\n MeanLossGraphCallback (after_create=None, before_fit=None,\n                        before_epoch=None, before_train=None,\n                        before_batch=None, after_pred=None,\n                        after_loss=None, before_backward=None,\n                        after_cancel_backward=None, after_backward=None,\n                        before_step=None, after_cancel_step=None,\n                        after_step=None, after_cancel_batch=None,\n                        after_batch=None, after_cancel_train=None,\n                        after_train=None, before_validate=None,\n                        after_cancel_validate=None, after_validate=None,\n                        after_cancel_epoch=None, after_epoch=None,\n                        after_cancel_fit=None, after_fit=None)\n\nUpdate a graph of training and validation loss\n\nsource\n\n\n\n\n ShortEpochCallback (pct=0.01, short_valid=True)\n\nFit just pct of an epoch, then stop\n\nsource\n\n\n\n\n GradientAccumulation (n_acc=32)\n\nAccumulate gradients before updating weights\n\nsource\n\n\n\n\n EarlyStoppingCallback (monitor='valid_loss', comp=None, min_delta=0.0,\n                        patience=1, reset_on_fit=True)\n\nA TrackerCallback that terminates training when monitored quantity stops improving.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\npatience\nint\n1\nnumber of epochs to wait when training has not improved model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\n\n\n SaveModelCallback (monitor='valid_loss', comp=None, min_delta=0.0,\n                    fname='model', every_epoch=False, at_end=False,\n                    with_opt=False, reset_on_fit=True)\n\nA TrackerCallback that saves the model’s best during training and loads it at the end.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\nfname\nstr\nmodel\nmodel name to be used when saving model.\n\n\nevery_epoch\nbool\nFalse\nif true, save model after every epoch; else save only when model is better than existing best.\n\n\nat_end\nbool\nFalse\nif true, save model when training ends; else load best model if there is only one saved model.\n\n\nwith_opt\nbool\nFalse\nif true, save optimizer state (if any available) when saving model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\n\n\n ReduceLROnPlateau (monitor='valid_loss', comp=None, min_delta=0.0,\n                    patience=1, factor=10.0, min_lr=0, reset_on_fit=True)\n\nA TrackerCallback that reduces learning rate when a metric has stopped improving.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\npatience\nint\n1\nnumber of epochs to wait when training has not improved model.\n\n\nfactor\nfloat\n10.0\nthe denominator to divide the learning rate by, when reducing the learning rate.\n\n\nmin_lr\nint\n0\nthe minimum learning rate allowed; learning rate cannot be reduced below this minimum.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss)."
  },
  {
    "objectID": "callbacks.html#training-callbacks",
    "href": "callbacks.html#training-callbacks",
    "title": "Callbacks",
    "section": "",
    "text": "Callbacks that add functionlities during the training phase, including Callbacks that make decisions depending how a monitored metric/loss behaves\n\nsource\n\n\n\n MeanLossGraphCallback (after_create=None, before_fit=None,\n                        before_epoch=None, before_train=None,\n                        before_batch=None, after_pred=None,\n                        after_loss=None, before_backward=None,\n                        after_cancel_backward=None, after_backward=None,\n                        before_step=None, after_cancel_step=None,\n                        after_step=None, after_cancel_batch=None,\n                        after_batch=None, after_cancel_train=None,\n                        after_train=None, before_validate=None,\n                        after_cancel_validate=None, after_validate=None,\n                        after_cancel_epoch=None, after_epoch=None,\n                        after_cancel_fit=None, after_fit=None)\n\nUpdate a graph of training and validation loss\n\nsource\n\n\n\n\n ShortEpochCallback (pct=0.01, short_valid=True)\n\nFit just pct of an epoch, then stop\n\nsource\n\n\n\n\n GradientAccumulation (n_acc=32)\n\nAccumulate gradients before updating weights\n\nsource\n\n\n\n\n EarlyStoppingCallback (monitor='valid_loss', comp=None, min_delta=0.0,\n                        patience=1, reset_on_fit=True)\n\nA TrackerCallback that terminates training when monitored quantity stops improving.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\npatience\nint\n1\nnumber of epochs to wait when training has not improved model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\n\n\n SaveModelCallback (monitor='valid_loss', comp=None, min_delta=0.0,\n                    fname='model', every_epoch=False, at_end=False,\n                    with_opt=False, reset_on_fit=True)\n\nA TrackerCallback that saves the model’s best during training and loads it at the end.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\nfname\nstr\nmodel\nmodel name to be used when saving model.\n\n\nevery_epoch\nbool\nFalse\nif true, save model after every epoch; else save only when model is better than existing best.\n\n\nat_end\nbool\nFalse\nif true, save model when training ends; else load best model if there is only one saved model.\n\n\nwith_opt\nbool\nFalse\nif true, save optimizer state (if any available) when saving model.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n\n\n\n\nsource\n\n\n\n\n ReduceLROnPlateau (monitor='valid_loss', comp=None, min_delta=0.0,\n                    patience=1, factor=10.0, min_lr=0, reset_on_fit=True)\n\nA TrackerCallback that reduces learning rate when a metric has stopped improving.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmonitor\nstr\nvalid_loss\nvalue (usually loss or metric) being monitored.\n\n\ncomp\nNoneType\nNone\nnumpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n\n\nmin_delta\nfloat\n0.0\nminimum delta between the last monitor value and the best monitor value.\n\n\npatience\nint\n1\nnumber of epochs to wait when training has not improved model.\n\n\nfactor\nfloat\n10.0\nthe denominator to divide the learning rate by, when reducing the learning rate.\n\n\nmin_lr\nint\n0\nthe minimum learning rate allowed; learning rate cannot be reduced below this minimum.\n\n\nreset_on_fit\nbool\nTrue\nbefore model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss)."
  },
  {
    "objectID": "callbacks.html#schedulers",
    "href": "callbacks.html#schedulers",
    "title": "Callbacks",
    "section": "Schedulers",
    "text": "Schedulers\nCallback and helper functions to schedule hyper-parameters\n\nsource\n\nParamScheduler\n\n ParamScheduler (scheds)\n\nSchedule hyper-parameters according to scheds\nscheds is a dictionary with one key for each hyper-parameter you want to schedule, with either a scheduler or a list of schedulers as values (in the second case, the list must have the same length as the the number of parameters groups of the optimizer).\n\nsource\n\n\nSchedCos\n\n SchedCos (start, end)\n\nCosine schedule function from start to end\n\nsource\n\n\nSchedExp\n\n SchedExp (start, end)\n\nExponential schedule function from start to end\n\nsource\n\n\nSchedLin\n\n SchedLin (start, end)\n\nLinear schedule function from start to end\n\nsource\n\n\nSchedNo\n\n SchedNo (start, end)\n\nConstant schedule function with start value"
  }
]